{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ B∆Ø·ªöC 0: C√†i ƒê·∫∑t v√† Import Th∆∞ Vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T07:59:06.494771Z",
     "iopub.status.busy": "2025-12-14T07:59:06.494460Z",
     "iopub.status.idle": "2025-12-14T07:59:12.368468Z",
     "shell.execute_reply": "2025-12-14T07:59:12.367773Z",
     "shell.execute_reply.started": "2025-12-14T07:59:06.494745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• ƒêang c√†i ƒë·∫∑t biopython...\n",
      "‚úÖ biopython c√†i ƒë·∫∑t xong\n",
      "‚úÖ lightgbm ƒë√£ c√†i ƒë·∫∑t\n",
      "üì• ƒêang c√†i ƒë·∫∑t scikit-learn...\n",
      "‚úÖ scikit-learn c√†i ƒë·∫∑t xong\n",
      "‚úÖ pandas ƒë√£ c√†i ƒë·∫∑t\n",
      "‚úÖ numpy ƒë√£ c√†i ƒë·∫∑t\n",
      "‚úÖ tqdm ƒë√£ c√†i ƒë·∫∑t\n",
      "\n",
      "‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán s·∫µn s√†ng!\n"
     ]
    }
   ],
   "source": [
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_needed(package):\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f'‚úÖ {package} ƒë√£ c√†i ƒë·∫∑t')\n",
    "    except ImportError:\n",
    "        print(f'üì• ƒêang c√†i ƒë·∫∑t {package}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
    "        print(f'‚úÖ {package} c√†i ƒë·∫∑t xong')\n",
    "\n",
    "# C√†i ƒë·∫∑t c√°c package c·∫ßn thi·∫øt\n",
    "packages = ['biopython', 'lightgbm', 'scikit-learn', 'pandas', 'numpy', 'tqdm']\n",
    "for pkg in packages:\n",
    "    install_if_needed(pkg)\n",
    "\n",
    "print('\\n‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán s·∫µn s√†ng!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T07:59:12.369837Z",
     "iopub.status.busy": "2025-12-14T07:59:12.369611Z",
     "iopub.status.idle": "2025-12-14T07:59:12.376272Z",
     "shell.execute_reply": "2025-12-14T07:59:12.375625Z",
     "shell.execute_reply.started": "2025-12-14T07:59:12.369820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import!\n",
      "üìä NumPy: 1.26.4 | Pandas: 2.2.3\n",
      "üî• PyTorch: 2.6.0+cu124 | Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import th∆∞ vi·ªán\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# BioPython\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import!')\n",
    "print(f'üìä NumPy: {np.__version__} | Pandas: {pd.__version__}')\n",
    "print(f'üî• PyTorch: {torch.__version__} | Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ B∆Ø·ªöC 1: Thi·∫øt L·∫≠p ƒê∆∞·ªùng D·∫´n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T07:59:12.377699Z",
     "iopub.status.busy": "2025-12-14T07:59:12.376938Z",
     "iopub.status.idle": "2025-12-14T07:59:12.398090Z",
     "shell.execute_reply": "2025-12-14T07:59:12.397418Z",
     "shell.execute_reply.started": "2025-12-14T07:59:12.377675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ƒêang ch·∫°y tr√™n Kaggle\n",
      "   Available inputs: ['cafa-6-protein-function-prediction']\n",
      "   Dataset dir: /kaggle/input/cafa-6-protein-function-prediction\n",
      "   Contents: ['sample_submission.tsv', 'IA.tsv', 'Test', 'Train']\n",
      "\n",
      "üìÅ Train: /kaggle/input/cafa-6-protein-function-prediction/Train\n",
      "üìÅ Test: /kaggle/input/cafa-6-protein-function-prediction/Test\n",
      "\n",
      "üìã File verification:\n",
      "   train_terms.tsv: ‚úÖ /kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\n",
      "   train_sequences.fasta: ‚úÖ /kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\n",
      "\n",
      "‚úÖ ƒê∆∞·ªùng d·∫´n ƒë√£ thi·∫øt l·∫≠p!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('/kaggle/input'):\n",
    "    print('üîç ƒêang ch·∫°y tr√™n Kaggle')\n",
    "    \n",
    "    input_base = Path('/kaggle/input')\n",
    "    print(f'   Available inputs: {os.listdir(input_base)}')\n",
    "    \n",
    "    # Find CAFA dataset\n",
    "    cafa_dir = None\n",
    "    for item in os.listdir(input_base):\n",
    "        if 'cafa' in item.lower():\n",
    "            cafa_dir = input_base / item\n",
    "            break\n",
    "    \n",
    "    if cafa_dir is None:\n",
    "        cafa_dir = input_base / os.listdir(input_base)[0]\n",
    "    \n",
    "    print(f'   Dataset dir: {cafa_dir}')\n",
    "    print(f'   Contents: {os.listdir(cafa_dir)}')\n",
    "    \n",
    "    if (cafa_dir / 'Train').exists():\n",
    "        TRAIN_PATH = cafa_dir / 'Train'\n",
    "        TEST_PATH = cafa_dir / 'Test'\n",
    "    else:\n",
    "        TRAIN_PATH = cafa_dir\n",
    "        TEST_PATH = cafa_dir\n",
    "    \n",
    "    IA_PATH = cafa_dir / 'IA.tsv'\n",
    "    OUTPUT_PATH = Path('/kaggle/working')\n",
    "    DATA_PATH = OUTPUT_PATH / 'data'\n",
    "    MODEL_PATH = OUTPUT_PATH / 'models'\n",
    "    SUBMISSION_PATH = OUTPUT_PATH  \n",
    "    \n",
    "else:\n",
    "    print('üîç ƒêang ch·∫°y Local')\n",
    "    TRAIN_PATH = Path('../Train')\n",
    "    TEST_PATH = Path('../Test')\n",
    "    IA_PATH = Path('../IA.tsv')\n",
    "    \n",
    "    DATA_PATH = Path('../data/processed')\n",
    "    MODEL_PATH = Path('../models')\n",
    "    SUBMISSION_PATH = Path('../submissions')\n",
    "\n",
    "for path in [DATA_PATH, MODEL_PATH, SUBMISSION_PATH]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'\\nüìÅ Train: {TRAIN_PATH}')\n",
    "print(f'üìÅ Test: {TEST_PATH}')\n",
    "\n",
    "print(f'\\nüìã File verification:')\n",
    "for fname in ['train_terms.tsv', 'train_sequences.fasta']:\n",
    "    fpath = TRAIN_PATH / fname\n",
    "    exists = fpath.exists()\n",
    "    print(f'   {fname}: {\"‚úÖ\" if exists else \"‚ùå\"} {fpath}')\n",
    "    \n",
    "    if not exists:\n",
    "        # Search for file\n",
    "        print(f'   üîç Searching for {fname}...')\n",
    "        for root, dirs, files in os.walk('/kaggle/input'):\n",
    "            if fname in files:\n",
    "                print(f'      Found: {root}/{fname}')\n",
    "                if 'train_terms' in fname:\n",
    "                    TRAIN_PATH = Path(root)\n",
    "\n",
    "print('\\n‚úÖ ƒê∆∞·ªùng d·∫´n ƒë√£ thi·∫øt l·∫≠p!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• B∆Ø·ªöC 2: T·∫£i D·ªØ Li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T07:59:12.400384Z",
     "iopub.status.busy": "2025-12-14T07:59:12.399750Z",
     "iopub.status.idle": "2025-12-14T07:59:15.225427Z",
     "shell.execute_reply": "2025-12-14T07:59:15.224750Z",
     "shell.execute_reply.started": "2025-12-14T07:59:12.400355Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìÇ LOADING DATA\n",
      "============================================================\n",
      "üìã Loading train_terms.tsv...\n",
      "   ‚úÖ 537,027 annotations\n",
      "   Columns: ['EntryID', 'term', 'aspect']\n",
      "   üìù Renamed columns: {'EntryID': 'protein_id', 'term': 'go_term'}\n",
      "   New columns: ['protein_id', 'go_term', 'aspect']\n",
      "\n",
      "üß¨ Loading train_sequences.fasta...\n",
      "   ‚úÖ 82,404 sequences\n",
      "   Sample FASTA IDs: ['A0A0C5B5G6', 'A0JNW5', 'A0JP26']\n",
      "   Sample train_terms IDs: ['Q5W0B1', 'Q5W0B1', 'Q5W0B1']\n",
      "\n",
      "üß™ Loading testsuperset.fasta...\n",
      "   ‚úÖ 224,309 test sequences\n",
      "\n",
      "‚öñÔ∏è Loading IA.tsv...\n",
      "   ‚úÖ 40,122 GO term weights\n",
      "\n",
      "üîç ID Matching Check:\n",
      "   train_terms IDs: 82,404\n",
      "   train_sequences IDs: 82,404\n",
      "   Overlapping IDs: 82,404\n",
      "   ‚úÖ All proteins have sequences!\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA LOADED SUCCESSFULLY!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('üìÇ LOADING DATA')\n",
    "print('='*60)\n",
    "\n",
    "def extract_protein_id(fasta_id):\n",
    "    \"\"\"\n",
    "    Extract UniProt accession t·ª´ FASTA ID\n",
    "    Handles multiple formats:\n",
    "    - \"sp|A0A0C5B5G6|MOTSC_HUMAN\" -> \"A0A0C5B5G6\"\n",
    "    - \"A0A0C5B5G6 9606\" -> \"A0A0C5B5G6\"\n",
    "    - \"A0A0C5B5G6\" -> \"A0A0C5B5G6\"\n",
    "    \"\"\"\n",
    "    fasta_id = fasta_id.split()[0]\n",
    "    \n",
    "    if '|' in fasta_id:\n",
    "        parts = fasta_id.split('|')\n",
    "        return parts[1] if len(parts) >= 2 else fasta_id\n",
    "    \n",
    "    return fasta_id\n",
    "\n",
    "# 1. Load annotations (train_terms.tsv)\n",
    "print('üìã Loading train_terms.tsv...')\n",
    "train_terms = pd.read_csv(TRAIN_PATH / 'train_terms.tsv', sep='\\t')\n",
    "print(f'   ‚úÖ {len(train_terms):,} annotations')\n",
    "print(f'   Columns: {list(train_terms.columns)}')\n",
    "\n",
    "# Standardize column names\n",
    "column_mapping = {}\n",
    "if 'EntryID' in train_terms.columns:\n",
    "    column_mapping['EntryID'] = 'protein_id'\n",
    "if 'term' in train_terms.columns:\n",
    "    column_mapping['term'] = 'go_term'\n",
    "\n",
    "if column_mapping:\n",
    "    train_terms = train_terms.rename(columns=column_mapping)\n",
    "    print(f'   üìù Renamed columns: {column_mapping}')\n",
    "    print(f'   New columns: {list(train_terms.columns)}')\n",
    "\n",
    "# 2. Load protein sequences\n",
    "print('\\nüß¨ Loading train_sequences.fasta...')\n",
    "train_sequences = {}\n",
    "for record in SeqIO.parse(TRAIN_PATH / 'train_sequences.fasta', 'fasta'):\n",
    "    accession = extract_protein_id(record.id)\n",
    "    train_sequences[accession] = str(record.seq)\n",
    "print(f'   ‚úÖ {len(train_sequences):,} sequences')\n",
    "\n",
    "# Debug: Show first few IDs\n",
    "print(f'   Sample FASTA IDs: {list(train_sequences.keys())[:3]}')\n",
    "print(f'   Sample train_terms IDs: {train_terms[\"protein_id\"].head(3).tolist()}')\n",
    "\n",
    "# 3. Load test sequences\n",
    "print('\\nüß™ Loading testsuperset.fasta...')\n",
    "test_sequences = {}\n",
    "for record in SeqIO.parse(TEST_PATH / 'testsuperset.fasta', 'fasta'):\n",
    "    accession = extract_protein_id(record.id)\n",
    "    test_sequences[accession] = str(record.seq)\n",
    "print(f'   ‚úÖ {len(test_sequences):,} test sequences')\n",
    "\n",
    "# 4. Load IA weights (n·∫øu c√≥)\n",
    "print('\\n‚öñÔ∏è Loading IA.tsv...')\n",
    "try:\n",
    "    ia_weights = pd.read_csv(IA_PATH, sep='\\t', header=None, names=['go_term', 'ia_weight'])\n",
    "    print(f'   ‚úÖ {len(ia_weights):,} GO term weights')\n",
    "except:\n",
    "    ia_weights = None\n",
    "    print('   ‚ö†Ô∏è IA.tsv not found')\n",
    "\n",
    "# Verify overlap\n",
    "train_ids = set(train_terms['protein_id'].unique())\n",
    "seq_ids = set(train_sequences.keys())\n",
    "overlap = train_ids & seq_ids\n",
    "print(f'\\nüîç ID Matching Check:')\n",
    "print(f'   train_terms IDs: {len(train_ids):,}')\n",
    "print(f'   train_sequences IDs: {len(seq_ids):,}')\n",
    "print(f'   Overlapping IDs: {len(overlap):,}')\n",
    "\n",
    "if len(overlap) == 0:\n",
    "    print('   ‚ö†Ô∏è WARNING: No matching IDs! Check ID format.')\n",
    "    print(f'\\n   Debug - train_terms samples: {list(train_ids)[:3]}')\n",
    "    print(f'   Debug - sequence samples: {list(seq_ids)[:3]}')\n",
    "elif len(overlap) < len(train_ids):\n",
    "    print(f'   ‚ö†Ô∏è Missing sequences for {len(train_ids) - len(overlap):,} proteins')\n",
    "else:\n",
    "    print('   ‚úÖ All proteins have sequences!')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ DATA LOADED SUCCESSFULLY!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T07:59:15.226342Z",
     "iopub.status.busy": "2025-12-14T07:59:15.226130Z",
     "iopub.status.idle": "2025-12-14T07:59:15.890118Z",
     "shell.execute_reply": "2025-12-14T07:59:15.889449Z",
     "shell.execute_reply.started": "2025-12-14T07:59:15.226305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üå≥ LOADING GO HIERARCHY\n",
      "============================================================\n",
      "üìÇ Loading: /kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\n",
      "   ‚úÖ GO terms: 40,122\n",
      "   ‚úÖ Obsolete terms: 7,979\n",
      "   üìä By namespace:\n",
      "      ‚Ä¢ biological_process: 25,950\n",
      "      ‚Ä¢ molecular_function: 10,131\n",
      "      ‚Ä¢ cellular_component: 4,040\n",
      "      ‚Ä¢ external: 1\n",
      "\n",
      "üîó Building ancestor cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building GO hierarchy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26125/26125 [00:00<00:00, 240142.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Average ancestors per term: 13.3\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  LOAD GO HIERARCHY (go-basic.obo)\n",
    "print('='*60)\n",
    "print('üå≥ LOADING GO HIERARCHY')\n",
    "print('='*60)\n",
    "\n",
    "def parse_obo_file(obo_path):\n",
    "    \"\"\"Parse GO OBO file to extract hierarchy\"\"\"\n",
    "    go_parents = {}  \n",
    "    go_names = {}    \n",
    "    go_namespace = {} \n",
    "    obsolete_terms = set()\n",
    "    \n",
    "    current_term = None\n",
    "    current_name = None\n",
    "    current_namespace = None\n",
    "    current_parents = []\n",
    "    is_obsolete = False\n",
    "    \n",
    "    with open(obo_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line == '[Term]':\n",
    "                # Save previous term\n",
    "                if current_term and not is_obsolete:\n",
    "                    go_parents[current_term] = current_parents\n",
    "                    go_names[current_term] = current_name\n",
    "                    go_namespace[current_term] = current_namespace\n",
    "                elif current_term and is_obsolete:\n",
    "                    obsolete_terms.add(current_term)\n",
    "                \n",
    "                # Reset\n",
    "                current_term = None\n",
    "                current_name = None\n",
    "                current_namespace = None\n",
    "                current_parents = []\n",
    "                is_obsolete = False\n",
    "                \n",
    "            elif line.startswith('id: GO:'):\n",
    "                current_term = line.split('id: ')[1].strip()\n",
    "                \n",
    "            elif line.startswith('name: '):\n",
    "                current_name = line.split('name: ')[1].strip()\n",
    "                \n",
    "            elif line.startswith('namespace: '):\n",
    "                current_namespace = line.split('namespace: ')[1].strip()\n",
    "                \n",
    "            elif line.startswith('is_a: GO:'):\n",
    "                parent = line.split('is_a: ')[1].split(' !')[0].strip()\n",
    "                current_parents.append(parent)\n",
    "                \n",
    "            elif line.startswith('relationship: part_of GO:'):\n",
    "                parent = line.split('part_of ')[1].split(' !')[0].strip()\n",
    "                current_parents.append(parent)\n",
    "                \n",
    "            elif line == 'is_obsolete: true':\n",
    "                is_obsolete = True\n",
    "        \n",
    "        # Save last term\n",
    "        if current_term and not is_obsolete:\n",
    "            go_parents[current_term] = current_parents\n",
    "            go_names[current_term] = current_name\n",
    "            go_namespace[current_term] = current_namespace\n",
    "    \n",
    "    return go_parents, go_names, go_namespace, obsolete_terms\n",
    "\n",
    "def get_all_ancestors(term, go_parents, cache=None):\n",
    "    \"\"\"Get all ancestors of a GO term (including itself)\"\"\"\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    \n",
    "    if term in cache:\n",
    "        return cache[term]\n",
    "    \n",
    "    ancestors = {term}\n",
    "    if term in go_parents:\n",
    "        for parent in go_parents[term]:\n",
    "            ancestors.update(get_all_ancestors(parent, go_parents, cache))\n",
    "    \n",
    "    cache[term] = ancestors\n",
    "    return ancestors\n",
    "\n",
    "def build_ancestor_cache(go_terms, go_parents):\n",
    "    \"\"\"Build ancestor cache for all GO terms\"\"\"\n",
    "    cache = {}\n",
    "    for term in tqdm(go_terms, desc='Building GO hierarchy'):\n",
    "        get_all_ancestors(term, go_parents, cache)\n",
    "    return cache\n",
    "\n",
    "# Load GO hierarchy\n",
    "obo_path = TRAIN_PATH / 'go-basic.obo'\n",
    "if obo_path.exists():\n",
    "    print(f'üìÇ Loading: {obo_path}')\n",
    "    go_parents, go_names, go_namespace, obsolete_terms = parse_obo_file(obo_path)\n",
    "    \n",
    "    print(f'   ‚úÖ GO terms: {len(go_parents):,}')\n",
    "    print(f'   ‚úÖ Obsolete terms: {len(obsolete_terms):,}')\n",
    "    \n",
    "    # Count by namespace\n",
    "    ns_counts = {}\n",
    "    for term, ns in go_namespace.items():\n",
    "        ns_counts[ns] = ns_counts.get(ns, 0) + 1\n",
    "    print(f'   üìä By namespace:')\n",
    "    for ns, count in ns_counts.items():\n",
    "        print(f'      ‚Ä¢ {ns}: {count:,}')\n",
    "    \n",
    "    # Build ancestor cache\n",
    "    print('\\nüîó Building ancestor cache...')\n",
    "    all_training_terms = set(train_terms['go_term'].unique())\n",
    "    ancestor_cache = build_ancestor_cache(all_training_terms, go_parents)\n",
    "    \n",
    "    # Count average ancestors\n",
    "    avg_ancestors = np.mean([len(ancestor_cache.get(t, {t})) for t in all_training_terms])\n",
    "    print(f'   ‚úÖ Average ancestors per term: {avg_ancestors:.1f}')\n",
    "    \n",
    "    GO_HIERARCHY_LOADED = True\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è go-basic.obo not found at {obo_path}')\n",
    "    GO_HIERARCHY_LOADED = False\n",
    "    go_parents = {}\n",
    "    ancestor_cache = {}\n",
    "\n",
    "print('\\n' + '='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä B∆Ø·ªöC 3: Ph√¢n T√≠ch D·ªØ Li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T07:59:15.891169Z",
     "iopub.status.busy": "2025-12-14T07:59:15.890929Z",
     "iopub.status.idle": "2025-12-14T07:59:16.037334Z",
     "shell.execute_reply": "2025-12-14T07:59:16.036721Z",
     "shell.execute_reply.started": "2025-12-14T07:59:15.891151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä DATA ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üß¨ Proteins: 82,404\n",
      "üìö Unique GO Terms: 26,125\n",
      "üìã Total Annotations: 537,027\n",
      "üìà Avg GO terms per protein: 6.52\n",
      "\n",
      "üìä GO Term Distribution:\n",
      "   ‚Ä¢ Rare (1-10 proteins): 18,868 terms\n",
      "   ‚Ä¢ Medium (11-100): 6,594 terms\n",
      "   ‚Ä¢ Common (>100): 663 terms\n",
      "   ‚Ä¢ Most common: GO:0005515 (33,713 proteins)\n",
      "\n",
      "üßµ Sequence Length Stats:\n",
      "   ‚Ä¢ Mean: 526\n",
      "   ‚Ä¢ Median: 409\n",
      "   ‚Ä¢ Min/Max: 3 / 35213\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('üìä DATA ANALYSIS')\n",
    "print('='*60)\n",
    "\n",
    "# Th·ªëng k√™ c∆° b·∫£n\n",
    "n_proteins = train_terms['protein_id'].nunique()\n",
    "n_go_terms = train_terms['go_term'].nunique()\n",
    "n_annotations = len(train_terms)\n",
    "\n",
    "print(f'\\nüß¨ Proteins: {n_proteins:,}')\n",
    "print(f'üìö Unique GO Terms: {n_go_terms:,}')\n",
    "print(f'üìã Total Annotations: {n_annotations:,}')\n",
    "print(f'üìà Avg GO terms per protein: {n_annotations / n_proteins:.2f}')\n",
    "\n",
    "# Ph√¢n b·ªë GO terms\n",
    "go_counts = train_terms['go_term'].value_counts()\n",
    "print(f'\\nüìä GO Term Distribution:')\n",
    "print(f'   ‚Ä¢ Rare (1-10 proteins): {(go_counts <= 10).sum():,} terms')\n",
    "print(f'   ‚Ä¢ Medium (11-100): {((go_counts > 10) & (go_counts <= 100)).sum():,} terms')\n",
    "print(f'   ‚Ä¢ Common (>100): {(go_counts > 100).sum():,} terms')\n",
    "print(f'   ‚Ä¢ Most common: {go_counts.index[0]} ({go_counts.iloc[0]:,} proteins)')\n",
    "\n",
    "# Sequence lengths\n",
    "if train_sequences:\n",
    "    seq_lengths = [len(seq) for seq in train_sequences.values()]\n",
    "    print(f'\\nüßµ Sequence Length Stats:')\n",
    "    print(f'   ‚Ä¢ Mean: {np.mean(seq_lengths):.0f}')\n",
    "    print(f'   ‚Ä¢ Median: {np.median(seq_lengths):.0f}')\n",
    "    print(f'   ‚Ä¢ Min/Max: {min(seq_lengths)} / {max(seq_lengths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è B∆Ø·ªöC 4: Feature Engineering\n",
    "\n",
    "T·∫°o c√°c ƒë·∫∑c tr∆∞ng t·ª´ chu·ªói protein:\n",
    "1. **Amino Acid Composition (AAC)**: T·ª∑ l·ªá 20 amino acids\n",
    "2. **Dipeptide Composition (DPC)**: T·ª∑ l·ªá 400 dipeptides\n",
    "3. **Physicochemical Properties**: C√°c t√≠nh ch·∫•t v·∫≠t l√Ω/h√≥a h·ªçc\n",
    "4. **Sequence Statistics**: Chi·ªÅu d√†i, entropy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T07:59:16.038441Z",
     "iopub.status.busy": "2025-12-14T07:59:16.038158Z",
     "iopub.status.idle": "2025-12-14T07:59:16.078053Z",
     "shell.execute_reply": "2025-12-14T07:59:16.077389Z",
     "shell.execute_reply.started": "2025-12-14T07:59:16.038417Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚öôÔ∏è FEATURE ENGINEERING (ENHANCED)\n",
      "============================================================\n",
      "\n",
      "üîß Feature vector size: 150\n",
      "   Sample features (first 10): [0.0016     0.28332132 0.016      0.         0.         0.\n",
      " 0.0625     0.0625     0.0625     0.        ]\n",
      "   Non-zero features: 83\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('‚öôÔ∏è FEATURE ENGINEERING (ENHANCED)')\n",
    "print('='*60)\n",
    "\n",
    "# Amino acids\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "# Physicochemical properties\n",
    "HYDROPHOBIC = set('AILMFVWP')\n",
    "HYDROPHILIC = set('RKDENQ')\n",
    "POLAR = set('STNQ')\n",
    "CHARGED_POS = set('RKH')\n",
    "CHARGED_NEG = set('DE')\n",
    "AROMATIC = set('FYW')\n",
    "SMALL = set('AGSTC')\n",
    "ALIPHATIC = set('ILV')\n",
    "SULFUR = set('CM')\n",
    "HYDROXYL = set('ST')\n",
    "\n",
    "# Molecular weights\n",
    "AA_MW = {'A': 89, 'C': 121, 'D': 133, 'E': 147, 'F': 165, 'G': 75, 'H': 155, \n",
    "         'I': 131, 'K': 146, 'L': 131, 'M': 149, 'N': 132, 'P': 115, 'Q': 146,\n",
    "         'R': 174, 'S': 105, 'T': 119, 'V': 117, 'W': 204, 'Y': 181}\n",
    "\n",
    "# Hydrophobicity scale (Kyte-Doolittle)\n",
    "HYDRO_SCALE = {'A': 1.8, 'C': 2.5, 'D': -3.5, 'E': -3.5, 'F': 2.8, 'G': -0.4,\n",
    "               'H': -3.2, 'I': 4.5, 'K': -3.9, 'L': 3.8, 'M': 1.9, 'N': -3.5,\n",
    "               'P': -1.6, 'Q': -3.5, 'R': -4.5, 'S': -0.8, 'T': -0.7, 'V': 4.2,\n",
    "               'W': -0.9, 'Y': -1.3}\n",
    "\n",
    "def extract_features_enhanced(sequence):\n",
    "    \"\"\"Extract comprehensive features from protein sequence\"\"\"\n",
    "    seq = ''.join(aa for aa in sequence.upper() if aa in AMINO_ACIDS)\n",
    "    length = len(seq)\n",
    "    \n",
    "    if length == 0:\n",
    "        return np.zeros(150)  # Updated feature size\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # 1. Sequence length features (3)\n",
    "    features.append(length / 10000)\n",
    "    features.append(np.log1p(length) / 10)\n",
    "    features.append(min(length, 1000) / 1000)  # Capped length\n",
    "    \n",
    "    # 2. Amino Acid Composition (20)\n",
    "    aa_counts = Counter(seq)\n",
    "    for aa in AMINO_ACIDS:\n",
    "        features.append(aa_counts.get(aa, 0) / length)\n",
    "    \n",
    "    # 3. Physicochemical properties (15)\n",
    "    features.append(sum(1 for aa in seq if aa in HYDROPHOBIC) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in HYDROPHILIC) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in POLAR) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in CHARGED_POS) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in CHARGED_NEG) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in AROMATIC) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in SMALL) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in ALIPHATIC) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in SULFUR) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in HYDROXYL) / length)\n",
    "    \n",
    "    # Net charge\n",
    "    pos_charge = sum(1 for aa in seq if aa in CHARGED_POS)\n",
    "    neg_charge = sum(1 for aa in seq if aa in CHARGED_NEG)\n",
    "    features.append((pos_charge - neg_charge) / length)\n",
    "    \n",
    "    # Molecular weight\n",
    "    mw = sum(AA_MW.get(aa, 110) for aa in seq)\n",
    "    features.append(mw / (length * 200))  # Normalized\n",
    "    \n",
    "    # Hydrophobicity stats\n",
    "    hydro_values = [HYDRO_SCALE.get(aa, 0) for aa in seq]\n",
    "    features.append(np.mean(hydro_values) / 5)\n",
    "    features.append(np.std(hydro_values) / 5)\n",
    "    features.append(np.max(hydro_values) / 5 if hydro_values else 0)\n",
    "    \n",
    "    # 4. Sequence complexity (3)\n",
    "    aa_freq = np.array([aa_counts.get(aa, 0) / length for aa in AMINO_ACIDS])\n",
    "    aa_freq = aa_freq[aa_freq > 0]\n",
    "    entropy = -np.sum(aa_freq * np.log2(aa_freq)) if len(aa_freq) > 0 else 0\n",
    "    features.append(entropy / 4.5)\n",
    "    \n",
    "    # Unique AA ratio\n",
    "    features.append(len(aa_counts) / 20)\n",
    "    \n",
    "    # Repeat ratio (consecutive same AA)\n",
    "    repeats = sum(1 for i in range(len(seq)-1) if seq[i] == seq[i+1])\n",
    "    features.append(repeats / max(1, length - 1))\n",
    "    \n",
    "    # 5. N-terminal and C-terminal signals (16)\n",
    "    for term_seq, name in [(seq[:50], 'N'), (seq[-50:], 'C')]:\n",
    "        term_len = len(term_seq)\n",
    "        if term_len > 0:\n",
    "            features.append(sum(1 for aa in term_seq if aa in HYDROPHOBIC) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in CHARGED_POS) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in CHARGED_NEG) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in POLAR) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in AROMATIC) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in SMALL) / term_len)\n",
    "            # Hydrophobicity\n",
    "            term_hydro = [HYDRO_SCALE.get(aa, 0) for aa in term_seq]\n",
    "            features.append(np.mean(term_hydro) / 5)\n",
    "            features.append(np.std(term_hydro) / 5 if len(term_hydro) > 1 else 0)\n",
    "        else:\n",
    "            features.extend([0] * 8)\n",
    "    \n",
    "    # 6. Dipeptide features (30 most informative)\n",
    "    important_dipeptides = [\n",
    "        'LL', 'AA', 'VV', 'GG', 'SS', 'EE', 'KK', 'AL', 'LA', 'EK',\n",
    "        'KE', 'DE', 'ED', 'RK', 'LE', 'EL', 'AE', 'EA', 'LK', 'KL',\n",
    "        'GS', 'SG', 'PP', 'PG', 'GP', 'VL', 'LV', 'IL', 'LI', 'TT'\n",
    "    ]\n",
    "    for dp in important_dipeptides:\n",
    "        count = seq.count(dp)\n",
    "        features.append(count / max(1, length - 1))\n",
    "    \n",
    "    # 7. Tripeptide features (20 key patterns)\n",
    "    important_tripeptides = [\n",
    "        'LLL', 'AAA', 'GGG', 'SSS', 'EEE', 'KKK', 'PPP', 'VVV',\n",
    "        'LEK', 'EKL', 'ALA', 'GAS', 'VAL', 'ILE', 'PRO', 'GLY',\n",
    "        'SER', 'THR', 'ASP', 'GLU'\n",
    "    ]\n",
    "    for tp in important_tripeptides:\n",
    "        count = seq.count(tp)\n",
    "        features.append(count / max(1, length - 2))\n",
    "    \n",
    "    # 8. Position-specific features (20)\n",
    "    # Split sequence into 5 regions\n",
    "    region_size = length // 5 if length >= 5 else length\n",
    "    for i in range(5):\n",
    "        start = i * region_size\n",
    "        end = start + region_size if i < 4 else length\n",
    "        region = seq[start:end]\n",
    "        if len(region) > 0:\n",
    "            features.append(sum(1 for aa in region if aa in HYDROPHOBIC) / len(region))\n",
    "            features.append(sum(1 for aa in region if aa in CHARGED_POS) / len(region))\n",
    "            features.append(sum(1 for aa in region if aa in CHARGED_NEG) / len(region))\n",
    "            features.append(sum(1 for aa in region if aa in AROMATIC) / len(region))\n",
    "        else:\n",
    "            features.extend([0] * 4)\n",
    "    \n",
    "    # 9. Pseudo Amino Acid Composition (23)\n",
    "    # Sequence-order correlation factors\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        if length > lag:\n",
    "            # Hydrophobicity correlation\n",
    "            h_corr = sum(HYDRO_SCALE.get(seq[i], 0) * HYDRO_SCALE.get(seq[i+lag], 0) \n",
    "                        for i in range(length - lag)) / (length - lag)\n",
    "            features.append(h_corr / 25)\n",
    "        else:\n",
    "            features.append(0)\n",
    "    \n",
    "    # Type I PseAAC features (18 more)\n",
    "    for d in [1, 2, 3]:\n",
    "        if length > d:\n",
    "            hydro_diff = sum(abs(HYDRO_SCALE.get(seq[i], 0) - HYDRO_SCALE.get(seq[i+d], 0)) \n",
    "                            for i in range(length - d)) / (length - d)\n",
    "            mw_diff = sum(abs(AA_MW.get(seq[i], 110) - AA_MW.get(seq[i+d], 110)) \n",
    "                         for i in range(length - d)) / (length - d)\n",
    "            features.append(hydro_diff / 10)\n",
    "            features.append(mw_diff / 200)\n",
    "            \n",
    "            # Charge transition\n",
    "            charge_trans = sum(1 for i in range(length - d) \n",
    "                              if (seq[i] in CHARGED_POS) != (seq[i+d] in CHARGED_POS)) / (length - d)\n",
    "            features.append(charge_trans)\n",
    "            \n",
    "            # Polarity transition\n",
    "            polar_trans = sum(1 for i in range(length - d)\n",
    "                             if (seq[i] in POLAR) != (seq[i+d] in POLAR)) / (length - d)\n",
    "            features.append(polar_trans)\n",
    "            \n",
    "            # Size transition\n",
    "            size_trans = sum(1 for i in range(length - d)\n",
    "                            if (seq[i] in SMALL) != (seq[i+d] in SMALL)) / (length - d)\n",
    "            features.append(size_trans)\n",
    "            \n",
    "            # Aromatic clustering\n",
    "            arom_cluster = sum(1 for i in range(length - d)\n",
    "                              if seq[i] in AROMATIC and seq[i+d] in AROMATIC) / (length - d)\n",
    "            features.append(arom_cluster)\n",
    "        else:\n",
    "            features.extend([0] * 6)\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# Test feature extraction\n",
    "test_seq = list(train_sequences.values())[0]\n",
    "test_features = extract_features_enhanced(test_seq)\n",
    "print(f'\\nüîß Feature vector size: {len(test_features)}')\n",
    "print(f'   Sample features (first 10): {test_features[:10]}')\n",
    "print(f'   Non-zero features: {np.sum(test_features != 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T07:59:16.079066Z",
     "iopub.status.busy": "2025-12-14T07:59:16.078830Z",
     "iopub.status.idle": "2025-12-14T08:02:55.430949Z",
     "shell.execute_reply": "2025-12-14T08:02:55.430289Z",
     "shell.execute_reply.started": "2025-12-14T07:59:16.079049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß¨ Extracting features for training proteins...\n",
      "   Proteins in train_terms: 82,404\n",
      "   Sequences available: 82,404\n",
      "   Proteins with sequences: 82,404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82404/82404 [03:34<00:00, 383.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Features extracted: (82404, 150)\n",
      "   Valid proteins: 82,404 / 82,404\n",
      "\n",
      "üìä Creating label matrix (vectorized)...\n",
      "‚úÖ Label matrix: (82404, 26125)\n",
      "   Positive labels: 537,027 (0.0249%)\n"
     ]
    }
   ],
   "source": [
    "# Extract features for all training proteins\n",
    "print('\\nüß¨ Extracting features for training proteins...')\n",
    "\n",
    "proteins = train_terms['protein_id'].unique()\n",
    "all_go_terms = train_terms['go_term'].unique()\n",
    "\n",
    "print(f'   Proteins in train_terms: {len(proteins):,}')\n",
    "print(f'   Sequences available: {len(train_sequences):,}')\n",
    "\n",
    "# Debug: Check matching\n",
    "matched = sum(1 for p in proteins if p in train_sequences)\n",
    "print(f'   Proteins with sequences: {matched:,}')\n",
    "\n",
    "if matched == 0:\n",
    "    print('\\n‚ùå ERROR: No proteins match! Checking IDs...')\n",
    "    print(f'   First 3 train_terms IDs: {proteins[:3].tolist()}')\n",
    "    print(f'   First 3 sequence IDs: {list(train_sequences.keys())[:3]}')\n",
    "    raise ValueError(\"No matching protein IDs between train_terms and train_sequences!\")\n",
    "\n",
    "# Extract features with ENHANCED function\n",
    "X_features = []\n",
    "valid_proteins = []\n",
    "\n",
    "for pid in tqdm(proteins, desc='Extracting features'):\n",
    "    seq = train_sequences.get(pid, '')\n",
    "    if seq:\n",
    "        X_features.append(extract_features_enhanced(seq))\n",
    "        valid_proteins.append(pid)\n",
    "\n",
    "X = np.array(X_features)\n",
    "print(f'\\n‚úÖ Features extracted: {X.shape}')\n",
    "print(f'   Valid proteins: {len(valid_proteins):,} / {len(proteins):,}')\n",
    "\n",
    "if len(valid_proteins) == 0:\n",
    "    raise ValueError(\"No features extracted! Check protein ID matching.\")\n",
    "\n",
    "# üìä T·∫°o label matrix - VECTORIZED\n",
    "print('\\nüìä Creating label matrix (vectorized)...')\n",
    "\n",
    "protein_to_idx = {p: i for i, p in enumerate(valid_proteins)}\n",
    "term_to_idx = {t: i for i, t in enumerate(all_go_terms)}\n",
    "\n",
    "# Vectorized approach - much faster than iterrows\n",
    "valid_mask = train_terms['protein_id'].isin(protein_to_idx)\n",
    "valid_terms_df = train_terms[valid_mask]\n",
    "\n",
    "row_indices = valid_terms_df['protein_id'].map(protein_to_idx).values\n",
    "col_indices = valid_terms_df['go_term'].map(term_to_idx).values\n",
    "\n",
    "y = np.zeros((len(valid_proteins), len(all_go_terms)), dtype=np.int8)\n",
    "y[row_indices, col_indices] = 1\n",
    "\n",
    "print(f'‚úÖ Label matrix: {y.shape}')\n",
    "print(f'   Positive labels: {y.sum():,} ({100*y.sum()/(y.shape[0]*y.shape[1]):.4f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:02:55.431857Z",
     "iopub.status.busy": "2025-12-14T08:02:55.431662Z",
     "iopub.status.idle": "2025-12-14T08:03:02.148451Z",
     "shell.execute_reply": "2025-12-14T08:03:02.147536Z",
     "shell.execute_reply.started": "2025-12-14T08:02:55.431831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìê Scaling features...\n",
      "‚úÖ Features scaled: (82404, 150)\n",
      "\n",
      "üìä GO Term Filtering:\n",
      "   ‚Ä¢ Total: 26,125\n",
      "   ‚Ä¢ With ‚â•20 proteins: 4,227\n",
      "‚úÖ Filtered: 4,227 GO terms\n",
      "\n",
      "‚öñÔ∏è Preparing IA weights for weighted loss...\n",
      "   IA weights range: [0.000, 7.435]\n",
      "\n",
      "‚öñÔ∏è Calculating class weights...\n",
      "   Class weights range: [0.50, 0.67]\n"
     ]
    }
   ],
   "source": [
    "# Scale Features & Filter GO Terms & Create IA Weights\n",
    "print('\\nüìê Scaling features...')\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(f'‚úÖ Features scaled: {X_scaled.shape}')\n",
    "\n",
    "# Filter GO terms\n",
    "MIN_PROTEINS_PER_TERM = 20 \n",
    "\n",
    "term_counts = y.sum(axis=0)\n",
    "valid_mask = term_counts >= MIN_PROTEINS_PER_TERM\n",
    "\n",
    "print(f'\\nüìä GO Term Filtering:')\n",
    "print(f'   ‚Ä¢ Total: {len(term_counts):,}')\n",
    "print(f'   ‚Ä¢ With ‚â•{MIN_PROTEINS_PER_TERM} proteins: {valid_mask.sum():,}')\n",
    "\n",
    "y_filtered = y[:, valid_mask]\n",
    "filtered_go_terms = all_go_terms[valid_mask]\n",
    "\n",
    "print(f'‚úÖ Filtered: {y_filtered.shape[1]:,} GO terms')\n",
    "\n",
    "# ‚öñÔ∏è Prepare IA Weights for Loss Function\n",
    "print('\\n‚öñÔ∏è Preparing IA weights for weighted loss...')\n",
    "\n",
    "if ia_weights is not None:\n",
    "    ia_dict = dict(zip(ia_weights['go_term'], ia_weights['ia_weight']))\n",
    "    term_ia_weights = np.array([ia_dict.get(t, 1.0) for t in filtered_go_terms])\n",
    "    # Normalize weights\n",
    "    term_ia_weights = term_ia_weights / term_ia_weights.mean()\n",
    "    print(f'   IA weights range: [{term_ia_weights.min():.3f}, {term_ia_weights.max():.3f}]')\n",
    "else:\n",
    "    term_ia_weights = np.ones(len(filtered_go_terms))\n",
    "    print('   Using uniform weights (IA.tsv not found)')\n",
    "\n",
    "#  Calculate Class Weights for Imbalanced Data\n",
    "print('\\n‚öñÔ∏è Calculating class weights...')\n",
    "pos_counts = y_filtered.sum(axis=0)\n",
    "neg_counts = y_filtered.shape[0] - pos_counts\n",
    "\n",
    "# Effective number of samples weighting\n",
    "beta = 0.9999\n",
    "pos_weights = (1 - beta) / (1 - np.power(beta, pos_counts + 1))\n",
    "neg_weights = (1 - beta) / (1 - np.power(beta, neg_counts + 1))\n",
    "\n",
    "# Combine with IA weights\n",
    "class_weights = (neg_weights / pos_weights) * term_ia_weights\n",
    "class_weights = np.clip(class_weights, 0.5, 50)  # Clip extreme values\n",
    "print(f'   Class weights range: [{class_weights.min():.2f}, {class_weights.max():.2f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ B∆Ø·ªöC 5: Training Neural Network\n",
    "\n",
    "S·ª≠ d·ª•ng Neural Network v·ªõi Residual Blocks cho multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:03:02.150974Z",
     "iopub.status.busy": "2025-12-14T08:03:02.150713Z",
     "iopub.status.idle": "2025-12-14T08:03:07.157205Z",
     "shell.execute_reply": "2025-12-14T08:03:07.156571Z",
     "shell.execute_reply.started": "2025-12-14T08:03:02.150953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ü§ñ MODEL TRAINING SETUP (MEMORY OPTIMIZED)\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è Configuration (Memory Optimized):\n",
      "   ‚Ä¢ Max GO terms: 1500\n",
      "   ‚Ä¢ Hidden dim: 512\n",
      "   ‚Ä¢ Epochs: 25\n",
      "   ‚Ä¢ Batch size: 256\n",
      "\n",
      "üìä Splitting data...\n",
      "   Train: 74,163 samples\n",
      "   Val: 8,241 samples\n",
      "   GO terms: 1,500\n",
      "   Positive labels in train: 308,091\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('ü§ñ MODEL TRAINING SETUP (MEMORY OPTIMIZED)')\n",
    "print('='*60)\n",
    "\n",
    "import gc\n",
    "\n",
    "# C·∫§U H√åNH\n",
    "MAX_GO_TERMS = 1500      \n",
    "HIDDEN_DIM = 512       \n",
    "BATCH_SIZE = 256         \n",
    "NUM_EPOCHS = 25          \n",
    "LEARNING_RATE = 5e-4     \n",
    "PROB_THRESHOLD = 0.01    \n",
    "\n",
    "print(f'\\n‚öôÔ∏è Configuration (Memory Optimized):')\n",
    "print(f'   ‚Ä¢ Max GO terms: {MAX_GO_TERMS}')\n",
    "print(f'   ‚Ä¢ Hidden dim: {HIDDEN_DIM}')\n",
    "print(f'   ‚Ä¢ Epochs: {NUM_EPOCHS}')\n",
    "print(f'   ‚Ä¢ Batch size: {BATCH_SIZE}')\n",
    "\n",
    "# Clear unused memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Train-validation split\n",
    "print('\\nüìä Splitting data...')\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y_filtered,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "print(f'   Train: {X_train.shape[0]:,} samples')\n",
    "print(f'   Val: {X_val.shape[0]:,} samples')\n",
    "\n",
    "# Select top GO terms\n",
    "term_counts = y_train.sum(axis=0)\n",
    "top_term_indices = np.argsort(term_counts)[-MAX_GO_TERMS:]\n",
    "y_train = y_train[:, top_term_indices]\n",
    "y_val = y_val[:, top_term_indices]\n",
    "filtered_go_terms_final = filtered_go_terms[top_term_indices]\n",
    "final_class_weights = class_weights[top_term_indices]\n",
    "\n",
    "# Free memory from filtered arrays\n",
    "del y_filtered\n",
    "gc.collect()\n",
    "\n",
    "print(f'   GO terms: {y_train.shape[1]:,}')\n",
    "print(f'   Positive labels in train: {y_train.sum():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:03:07.158175Z",
     "iopub.status.busy": "2025-12-14T08:03:07.157988Z",
     "iopub.status.idle": "2025-12-14T08:05:31.057193Z",
     "shell.execute_reply": "2025-12-14T08:05:31.056473Z",
     "shell.execute_reply.started": "2025-12-14T08:03:07.158160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèãÔ∏è Training Neural Network (Improved)...\n",
      "   Using device: cuda\n",
      "   Model parameters: 2,838,364\n",
      "   Epoch  5/25 | Train: 0.0018 | Val: 0.0018 | LR: 0.000250\n",
      "   Epoch 10/25 | Train: 0.0018 | Val: 0.0017 | LR: 0.000500\n",
      "   Epoch 15/25 | Train: 0.0016 | Val: 0.0016 | LR: 0.000427\n",
      "   Epoch 20/25 | Train: 0.0015 | Val: 0.0015 | LR: 0.000250\n",
      "   Epoch 25/25 | Train: 0.0014 | Val: 0.0015 | LR: 0.000073\n",
      "\n",
      "‚úÖ Training completed in 140.9s (2.3 min)\n",
      "   Best Val Loss: 0.0015\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Model (IMPROVED)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
    "    def __init__(self, dim, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, dim // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim // reduction, dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        scale = self.fc(x)\n",
    "        return x * scale\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.GELU(), \n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.se = SEBlock(dim)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out = self.se(out)\n",
    "        return self.act(out + residual)\n",
    "\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection with wider first layer\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(hidden_dim, 0.25),\n",
    "            ResidualBlock(hidden_dim, 0.25),\n",
    "            ResidualBlock(hidden_dim, 0.25),\n",
    "            ResidualBlock(hidden_dim, 0.20),\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.res_blocks(x)\n",
    "        return self.output(x)\n",
    "\n",
    "# üì¶ Focal Loss for Imbalanced Multi-Label Classification\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none', pos_weight=self.pos_weight\n",
    "        )\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# üì¶ Prepare DataLoaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train), \n",
    "    torch.FloatTensor(y_train)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val), \n",
    "    torch.FloatTensor(y_val)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# üèãÔ∏è Training with Improved Setup\n",
    "print('\\nüèãÔ∏è Training Neural Network (Improved)...')\n",
    "print(f'   Using device: {device}')\n",
    "\n",
    "model = ProteinClassifier(X_train.shape[1], HIDDEN_DIM, y_train.shape[1]).to(device)\n",
    "print(f'   Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "# Optimizer with gradient clipping\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.02)\n",
    "\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# Focal Loss with class weights\n",
    "pos_weight_tensor = torch.FloatTensor(final_class_weights).to(device)\n",
    "criterion = FocalLoss(alpha=1.0, gamma=2.0, pos_weight=pos_weight_tensor)\n",
    "\n",
    "start_time = time.time()\n",
    "best_val_loss = float('inf')\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            val_loss += criterion(outputs, y_batch).item()\n",
    "    \n",
    "    avg_train = train_loss / len(train_loader)\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_val < best_val_loss:\n",
    "        best_val_loss = avg_val\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "        print(f'   Epoch {epoch+1:2d}/{NUM_EPOCHS} | Train: {avg_train:.4f} | Val: {avg_val:.4f} | LR: {current_lr:.6f}')\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'\\n   ‚ö†Ô∏è Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f'\\n‚úÖ Training completed in {training_time:.1f}s ({training_time/60:.1f} min)')\n",
    "print(f'   Best Val Loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:05:31.058558Z",
     "iopub.status.busy": "2025-12-14T08:05:31.057968Z",
     "iopub.status.idle": "2025-12-14T08:05:50.411228Z",
     "shell.execute_reply": "2025-12-14T08:05:50.410559Z",
     "shell.execute_reply.started": "2025-12-14T08:05:31.058537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Evaluating model...\n",
      "\n",
      "üéØ Finding optimal threshold...\n",
      "   Threshold 0.005: F1=0.0056, Avg predictions=1475.6\n",
      "   Threshold 0.010: F1=0.0059, Avg predictions=1384.7\n",
      "   Threshold 0.020: F1=0.0073, Avg predictions=1108.8\n",
      "   Threshold 0.030: F1=0.0096, Avg predictions=835.6\n",
      "   Threshold 0.050: F1=0.0174, Avg predictions=429.9\n",
      "   Threshold 0.070: F1=0.0319, Avg predictions=209.3\n",
      "   Threshold 0.100: F1=0.0712, Avg predictions=74.9\n",
      "\n",
      "‚úÖ Best threshold: 0.1 (F1=0.0712)\n",
      "\n",
      "üìä Final Results (threshold=0.1):\n",
      "   Train Hamming Loss: 0.048988\n",
      "   Val Hamming Loss: 0.048920\n",
      "   Val F1 (micro): 0.0712\n",
      "   Val Predictions: 617,214 positives (4.99%)\n",
      "   Unique GO terms predicted: 1,500\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model & Find Optimal Threshold\n",
    "print('\\nüìà Evaluating model...')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def predict_batch(X):\n",
    "    \"\"\"Predict probabilities with model\"\"\"\n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# Get predictions\n",
    "val_probs = predict_batch(X_val)\n",
    "train_probs = predict_batch(X_train)\n",
    "\n",
    "# Find Optimal Threshold using F1\n",
    "print('\\nüéØ Finding optimal threshold...')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0.01\n",
    "\n",
    "for threshold in [0.005, 0.01, 0.02, 0.03, 0.05, 0.07, 0.1]:\n",
    "    val_preds = (val_probs >= threshold).astype(np.int8)\n",
    "    \n",
    "    # Skip if no predictions\n",
    "    if val_preds.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    # Micro F1 (what competition uses)\n",
    "    f1_micro = f1_score(y_val, val_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    # Coverage (how many GO terms predicted)\n",
    "    coverage = val_preds.sum(axis=1).mean()\n",
    "    \n",
    "    print(f'   Threshold {threshold:.3f}: F1={f1_micro:.4f}, Avg predictions={coverage:.1f}')\n",
    "    \n",
    "    if f1_micro > best_f1:\n",
    "        best_f1 = f1_micro\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f'\\n‚úÖ Best threshold: {best_threshold} (F1={best_f1:.4f})')\n",
    "PROB_THRESHOLD = best_threshold\n",
    "\n",
    "# Final evaluation with best threshold\n",
    "val_preds = (val_probs >= PROB_THRESHOLD).astype(np.int8)\n",
    "train_preds = (train_probs >= PROB_THRESHOLD).astype(np.int8)\n",
    "\n",
    "val_hamming = hamming_loss(y_val, val_preds)\n",
    "train_hamming = hamming_loss(y_train, train_preds)\n",
    "\n",
    "print(f'\\nüìä Final Results (threshold={PROB_THRESHOLD}):')\n",
    "print(f'   Train Hamming Loss: {train_hamming:.6f}')\n",
    "print(f'   Val Hamming Loss: {val_hamming:.6f}')\n",
    "print(f'   Val F1 (micro): {best_f1:.4f}')\n",
    "print(f'   Val Predictions: {val_preds.sum():,} positives ({100*val_preds.mean():.2f}%)')\n",
    "print(f'   Unique GO terms predicted: {(val_preds.sum(axis=0) > 0).sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ B∆Ø·ªöC 6: Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:05:50.412277Z",
     "iopub.status.busy": "2025-12-14T08:05:50.412018Z",
     "iopub.status.idle": "2025-12-14T08:13:51.717174Z",
     "shell.execute_reply": "2025-12-14T08:13:51.716340Z",
     "shell.execute_reply.started": "2025-12-14T08:05:50.412248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ GENERATING TEST PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "üß¨ Extracting test features (enhanced)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [08:00<00:00, 466.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test features: (224309, 150)\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('üéØ GENERATING TEST PREDICTIONS')\n",
    "print('='*60)\n",
    "\n",
    "# Extract features for test proteins with ENHANCED function\n",
    "print('\\nüß¨ Extracting test features (enhanced)...')\n",
    "\n",
    "test_protein_ids = list(test_sequences.keys())\n",
    "X_test_features = []\n",
    "\n",
    "for pid in tqdm(test_protein_ids, desc='Extracting test features'):\n",
    "    X_test_features.append(extract_features_enhanced(test_sequences[pid]))\n",
    "\n",
    "X_test = np.array(X_test_features)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'‚úÖ Test features: {X_test_scaled.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:13:51.718559Z",
     "iopub.status.busy": "2025-12-14T08:13:51.718281Z",
     "iopub.status.idle": "2025-12-14T08:13:53.827564Z",
     "shell.execute_reply": "2025-12-14T08:13:53.826914Z",
     "shell.execute_reply.started": "2025-12-14T08:13:51.718540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:01<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions shape: (224309, 1500)\n"
     ]
    }
   ],
   "source": [
    "#  Make Predictions (Batched)\n",
    "print('\\nüöÄ Making predictions...')\n",
    "\n",
    "model.eval()\n",
    "batch_size = 10000\n",
    "all_probs = []\n",
    "\n",
    "for i in tqdm(range(0, len(X_test_scaled), batch_size), desc='Predicting'):\n",
    "    batch_X = X_test_scaled[i:i+batch_size]\n",
    "    X_tensor = torch.FloatTensor(batch_X).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "    all_probs.append(probs)\n",
    "\n",
    "y_test_proba = np.vstack(all_probs)\n",
    "print(f'‚úÖ Predictions shape: {y_test_proba.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù B∆Ø·ªöC 7: Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:13:53.828616Z",
     "iopub.status.busy": "2025-12-14T08:13:53.828310Z",
     "iopub.status.idle": "2025-12-14T08:14:37.378981Z",
     "shell.execute_reply": "2025-12-14T08:14:37.378391Z",
     "shell.execute_reply.started": "2025-12-14T08:13:53.828581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù CREATING SUBMISSION FILE\n",
      "============================================================\n",
      "\n",
      "üå≥ Applying GO Hierarchy Propagation...\n",
      "   Pre-computing mappings...\n",
      "   Model GO terms: 1,500\n",
      "   Threshold: 0.03, Max per protein: 100\n",
      "   Processing 224,309 proteins...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating submission: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [00:43<00:00, 5182.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Results:\n",
      "   Total predictions: 15,724,114\n",
      "   Unique GO terms: 2,558\n",
      "   Avg per protein: 70.1\n",
      "\n",
      "üîç Top 10 GO terms:\n",
      "   1. GO:0110165: 224,309 (100.0%) - cellular anatomical structure\n",
      "   2. GO:0005488: 224,309 (100.0%) - binding\n",
      "   3. GO:0005515: 224,230 (100.0%) - protein binding\n",
      "   4. GO:0005737: 221,477 (98.7%) - cytoplasm\n",
      "   5. GO:0043226: 217,812 (97.1%) - organelle\n",
      "   6. GO:0043227: 217,385 (96.9%) - membrane-bounded organelle\n",
      "   7. GO:0043229: 214,928 (95.8%) - intracellular organelle\n",
      "   8. GO:0043231: 214,834 (95.8%) - intracellular membrane-bounded orga\n",
      "   9. GO:0005829: 213,551 (95.2%) - cytosol\n",
      "   10. GO:0016020: 194,891 (86.9%) - membrane\n",
      "\n",
      "‚úÖ Submission saved: /kaggle/working/submission.tsv\n",
      "   Size: 406.00 MB\n"
     ]
    }
   ],
   "source": [
    "# CREATE SUBMISSION WITH GO HIERARCHY PROPAGATION\n",
    "print('='*60)\n",
    "print('üìù CREATING SUBMISSION FILE')\n",
    "print('='*60)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "submission_file = SUBMISSION_PATH / 'submission.tsv'\n",
    "propagated_file = SUBMISSION_PATH / 'submission_propagated.tsv'\n",
    "\n",
    "for f in [submission_file, propagated_file]:\n",
    "    if f.exists():\n",
    "        os.remove(f)\n",
    "        print(f'   üóëÔ∏è ƒê√£ x√≥a file c≈©: {f}')\n",
    "\n",
    "# Root terms that are too general\n",
    "ROOT_TERMS = {\n",
    "    'GO:0008150',  # biological_process\n",
    "    'GO:0003674',  # molecular_function  \n",
    "    'GO:0005575',  # cellular_component\n",
    "    'GO:0005623',  # cell\n",
    "    'GO:0044464',  # cell part\n",
    "    'GO:0044424',  # intracellular part\n",
    "    'GO:0005622',  # intracellular\n",
    "}\n",
    "\n",
    "# Very common terms to down-weight\n",
    "COMMON_GENERAL_TERMS = {\n",
    "    'GO:0005886',  # plasma membrane\n",
    "    'GO:0005737',  # cytoplasm\n",
    "    'GO:0005634',  # nucleus\n",
    "    'GO:0016020',  # membrane\n",
    "    'GO:0005829',  # cytosol\n",
    "    'GO:0005515',  # protein binding\n",
    "}\n",
    "\n",
    "#  GO HIERARCHY PROPAGATION\n",
    "if GO_HIERARCHY_LOADED:\n",
    "    print('\\nüå≥ Applying GO Hierarchy Propagation...')\n",
    "    \n",
    "    # Build IA dictionary from ia_weights DataFrame\n",
    "    if ia_weights is not None:\n",
    "        ia_dict = dict(zip(ia_weights['go_term'], ia_weights['ia_weight']))\n",
    "        max_ia = max(ia_dict.values()) if ia_dict else 1.0\n",
    "    else:\n",
    "        ia_dict = {}\n",
    "        max_ia = 1.0\n",
    "    \n",
    "    # Build filtered ancestors (exclude root terms)\n",
    "    print('   Pre-computing mappings...')\n",
    "    term_ancestors_filtered = {}\n",
    "    for term in filtered_go_terms_final:\n",
    "        ancestors = ancestor_cache.get(term, {term})\n",
    "        filtered_ancestors = ancestors - ROOT_TERMS\n",
    "        term_ancestors_filtered[term] = filtered_ancestors\n",
    "    \n",
    "    print(f'   Model GO terms: {len(filtered_go_terms_final):,}')\n",
    "    \n",
    "    # Precompute weights for common terms\n",
    "    all_terms_in_ancestors = set()\n",
    "    for ancestors in term_ancestors_filtered.values():\n",
    "        all_terms_in_ancestors.update(ancestors)\n",
    "    \n",
    "    term_weights = {t: 0.7 if t in COMMON_GENERAL_TERMS else 1.0 \n",
    "                   for t in all_terms_in_ancestors}\n",
    "    \n",
    "    # Precompute IA weights\n",
    "    if ia_dict:\n",
    "        term_ia_final = {t: ia_dict.get(t, 1.0) / max_ia \n",
    "                        for t in all_terms_in_ancestors}\n",
    "    else:\n",
    "        term_ia_final = None\n",
    "    \n",
    "    PROPAGATION_THRESHOLD = 0.03\n",
    "    MAX_PREDS_PROPAGATED = 100\n",
    "    \n",
    "    print(f'   Threshold: {PROPAGATION_THRESHOLD}, Max per protein: {MAX_PREDS_PROPAGATED}')\n",
    "    print(f'   Processing {len(test_protein_ids):,} proteins...\\n')\n",
    "    \n",
    "    total_predictions = 0\n",
    "    go_term_counts = {}\n",
    "    \n",
    "    # Process all proteins and write directly\n",
    "    with open(propagated_file, 'w') as f:\n",
    "        for i in tqdm(range(len(test_protein_ids)), desc='Creating submission', mininterval=0.5):\n",
    "            protein_id = test_protein_ids[i]\n",
    "            probs = y_test_proba[i]\n",
    "            \n",
    "            # Find top predictions\n",
    "            top_indices = np.argsort(-probs)[:20]\n",
    "            \n",
    "            # Filter by threshold or keep top 5\n",
    "            mask = probs[top_indices] >= PROPAGATION_THRESHOLD\n",
    "            if mask.sum() > 0:\n",
    "                top_indices = top_indices[mask]\n",
    "            else:\n",
    "                top_indices = top_indices[:5]\n",
    "            \n",
    "            # Propagate to ancestors\n",
    "            term_scores = {}\n",
    "            for idx in top_indices:\n",
    "                term = filtered_go_terms_final[idx]\n",
    "                prob = float(probs[idx])\n",
    "                \n",
    "                # Add term and ancestors\n",
    "                for ancestor in term_ancestors_filtered.get(term, {term}):\n",
    "                    weight = term_weights.get(ancestor, 1.0)\n",
    "                    weighted_prob = prob * weight\n",
    "                    \n",
    "                    if ancestor not in term_scores or weighted_prob > term_scores[ancestor]:\n",
    "                        term_scores[ancestor] = weighted_prob\n",
    "            \n",
    "            # Get top predictions\n",
    "            sorted_items = sorted(term_scores.items(), key=lambda x: -x[1])[:MAX_PREDS_PROPAGATED]\n",
    "            \n",
    "            # Write lines\n",
    "            for term, prob in sorted_items:\n",
    "                # Apply IA weight\n",
    "                if term_ia_final:\n",
    "                    ia_w = term_ia_final.get(term, 1.0)\n",
    "                    final_prob = prob * (0.7 + 0.3 * ia_w)\n",
    "                else:\n",
    "                    final_prob = prob\n",
    "                \n",
    "                f.write(f'{protein_id}\\t{term}\\t{final_prob:.6f}\\n')\n",
    "                total_predictions += 1\n",
    "                go_term_counts[term] = go_term_counts.get(term, 0) + 1\n",
    "    \n",
    "    print(f'\\nüìä Results:')\n",
    "    print(f'   Total predictions: {total_predictions:,}')\n",
    "    print(f'   Unique GO terms: {len(go_term_counts):,}')\n",
    "    print(f'   Avg per protein: {total_predictions / len(test_protein_ids):.1f}')\n",
    "    \n",
    "    # Show top GO terms\n",
    "    print(f'\\nüîç Top 10 GO terms:')\n",
    "    for i, (term, count) in enumerate(sorted(go_term_counts.items(), key=lambda x: -x[1])[:10], 1):\n",
    "        name = go_names.get(term, '')[:35]\n",
    "        pct = 100 * count / len(test_protein_ids)\n",
    "        print(f'   {i}. {term}: {count:,} ({pct:.1f}%) - {name}')\n",
    "    \n",
    "    # Copy to final submission\n",
    "    shutil.copy(propagated_file, submission_file)\n",
    "    \n",
    "else:\n",
    "    # No GO hierarchy - create simple submission\n",
    "    print('\\n‚ö†Ô∏è GO hierarchy not loaded - creating simple submission...')\n",
    "    \n",
    "    SIMPLE_THRESHOLD = 0.03\n",
    "    MAX_PREDS = 100\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with open(submission_file, 'w') as f:\n",
    "        for i in tqdm(range(len(test_protein_ids)), desc='Creating submission'):\n",
    "            protein_id = test_protein_ids[i]\n",
    "            probs = y_test_proba[i]\n",
    "            \n",
    "            top_indices = np.argsort(-probs)[:MAX_PREDS]\n",
    "            mask = probs[top_indices] >= SIMPLE_THRESHOLD\n",
    "            if mask.sum() > 0:\n",
    "                top_indices = top_indices[mask]\n",
    "            else:\n",
    "                top_indices = top_indices[:5]\n",
    "            \n",
    "            for idx in top_indices:\n",
    "                term = filtered_go_terms_final[idx]\n",
    "                prob = float(probs[idx])\n",
    "                f.write(f'{protein_id}\\t{term}\\t{prob:.6f}\\n')\n",
    "                total_predictions += 1\n",
    "    \n",
    "    print(f'   Total predictions: {total_predictions:,}')\n",
    "\n",
    "# Verify file\n",
    "file_size = os.path.getsize(submission_file) / (1024 * 1024)\n",
    "print(f'\\n‚úÖ Submission saved: {submission_file}')\n",
    "print(f'   Size: {file_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:14:37.379904Z",
     "iopub.status.busy": "2025-12-14T08:14:37.379715Z",
     "iopub.status.idle": "2025-12-14T08:14:41.814959Z",
     "shell.execute_reply": "2025-12-14T08:14:41.814196Z",
     "shell.execute_reply.started": "2025-12-14T08:14:37.379888Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Submission Preview:\n",
      "protein_id    go_term  confidence\n",
      "A0A0C5B5G6 GO:0043231    0.267249\n",
      "A0A0C5B5G6 GO:0043227    0.267981\n",
      "A0A0C5B5G6 GO:0043226    0.270981\n",
      "A0A0C5B5G6 GO:0005739    0.281662\n",
      "A0A0C5B5G6 GO:0043229    0.267155\n",
      "A0A0C5B5G6 GO:0110165    0.267185\n",
      "A0A0C5B5G6 GO:0005743    0.235178\n",
      "A0A0C5B5G6 GO:0005740    0.235892\n",
      "A0A0C5B5G6 GO:0031090    0.239600\n",
      "A0A0C5B5G6 GO:0031966    0.235190\n",
      "\n",
      "üèÜ Top GO Terms in submission:\n",
      "   1. GO:0005488: 224,309 (binding...)\n",
      "   2. GO:0110165: 224,309 (cellular anatomical structure...)\n",
      "   3. GO:0005515: 224,230 (protein binding...)\n",
      "   4. GO:0005737: 221,477 (cytoplasm...)\n",
      "   5. GO:0043226: 217,812 (organelle...)\n",
      "   6. GO:0043227: 217,385 (membrane-bounded organelle...)\n",
      "   7. GO:0043229: 214,928 (intracellular organelle...)\n",
      "   8. GO:0043231: 214,834 (intracellular membrane-bounded organelle...)\n",
      "   9. GO:0005829: 213,551 (cytosol...)\n",
      "   10. GO:0016020: 194,891 (membrane...)\n"
     ]
    }
   ],
   "source": [
    "# Preview submission\n",
    "print('\\nüìÑ Submission Preview:')\n",
    "preview_df = pd.read_csv(submission_file, sep='\\t', header=None, names=['protein_id', 'go_term', 'confidence'], nrows=10)\n",
    "print(preview_df.to_string(index=False))\n",
    "\n",
    "print('\\nüèÜ Top GO Terms in submission:')\n",
    "# Read just go_term column for stats\n",
    "go_counts = pd.read_csv(submission_file, sep='\\t', header=None, usecols=[1], names=['go_term'])['go_term'].value_counts().head(10)\n",
    "for i, (term, count) in enumerate(go_counts.items(), 1):\n",
    "    name = go_names.get(term, 'Unknown') if GO_HIERARCHY_LOADED else ''\n",
    "    print(f'   {i}. {term}: {count:,} ({name[:40]}...)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ B∆Ø·ªöC 8: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:14:41.816959Z",
     "iopub.status.busy": "2025-12-14T08:14:41.815802Z",
     "iopub.status.idle": "2025-12-14T08:14:41.822110Z",
     "shell.execute_reply": "2025-12-14T08:14:41.821484Z",
     "shell.execute_reply.started": "2025-12-14T08:14:41.816929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ PIPELINE COMPLETE!\n",
      "============================================================\n",
      "\n",
      "üìä SUMMARY:\n",
      "==================================================\n",
      "\n",
      "üìÅ Data:\n",
      "   ‚Ä¢ Training proteins: 82,404\n",
      "   ‚Ä¢ Test proteins: 224,309\n",
      "   ‚Ä¢ GO terms trained: 1,500\n",
      "\n",
      "üß¨ Features: 150 dimensions\n",
      "   ‚Ä¢ AAC, Physicochemical, Dipeptides, PseAAC\n",
      "\n",
      "ü§ñ Model: Neural Network\n",
      "   ‚Ä¢ Hidden: 512, ResBlocks: 4\n",
      "   ‚Ä¢ Focal Loss + IA Weights\n",
      "   ‚Ä¢ Training time: 2.3 min\n",
      "\n",
      "üå≥ GO Propagation: ‚úÖ ENABLED\n",
      "\n",
      "üìù Submission:\n",
      "   ‚Ä¢ File: /kaggle/working/submission.tsv\n",
      "   ‚Ä¢ Size: 406.0 MB\n",
      "\n",
      "==================================================\n",
      "‚úÖ Ready for Kaggle submission!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('üéâ PIPELINE COMPLETE!')\n",
    "print('='*60)\n",
    "\n",
    "# Get stats from file\n",
    "import os\n",
    "file_size = os.path.getsize(submission_file) / (1024 * 1024)  # MB\n",
    "\n",
    "print(f'''\n",
    "üìä SUMMARY:\n",
    "{'='*50}\n",
    "\n",
    "üìÅ Data:\n",
    "   ‚Ä¢ Training proteins: {len(valid_proteins):,}\n",
    "   ‚Ä¢ Test proteins: {len(test_protein_ids):,}\n",
    "   ‚Ä¢ GO terms trained: {len(filtered_go_terms_final):,}\n",
    "\n",
    "üß¨ Features: {X_scaled.shape[1]} dimensions\n",
    "   ‚Ä¢ AAC, Physicochemical, Dipeptides, PseAAC\n",
    "\n",
    "ü§ñ Model: Neural Network\n",
    "   ‚Ä¢ Hidden: {HIDDEN_DIM}, ResBlocks: 4\n",
    "   ‚Ä¢ Focal Loss + IA Weights\n",
    "   ‚Ä¢ Training time: {training_time/60:.1f} min\n",
    "\n",
    "üå≥ GO Propagation: {\"‚úÖ ENABLED\" if GO_HIERARCHY_LOADED else \"‚ùå\"}\n",
    "\n",
    "üìù Submission:\n",
    "   ‚Ä¢ File: {submission_file}\n",
    "   ‚Ä¢ Size: {file_size:.1f} MB\n",
    "\n",
    "{'='*50}\n",
    "‚úÖ Ready for Kaggle submission!\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:24:54.661294Z",
     "iopub.status.busy": "2025-12-14T08:24:54.660598Z",
     "iopub.status.idle": "2025-12-14T08:24:54.666583Z",
     "shell.execute_reply": "2025-12-14T08:24:54.665995Z",
     "shell.execute_reply.started": "2025-12-14T08:24:54.661268Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission.tsv' target='_blank'>submission.tsv</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/submission.tsv"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('submission.tsv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a31bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T01:15:26.173186Z",
     "iopub.status.busy": "2025-12-02T01:15:26.172916Z",
     "iopub.status.idle": "2025-12-02T01:16:31.219227Z",
     "shell.execute_reply": "2025-12-02T01:16:31.218338Z"
    },
    "papermill": {
     "duration": 65.050724,
     "end_time": "2025-12-02T01:16:31.220661",
     "exception": false,
     "start_time": "2025-12-02T01:15:26.169937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ› ï¸ CHáº¾ Äá»˜ Sá»¬A Lá»–I KHá»šP ID (DEBUG & FIX) ---\n",
      "[1/4] Äá»c vÃ  Soi dá»¯ liá»‡u...\n",
      "-> Äang Ä‘á»c file: train_sequences.fasta...\n",
      "   + 5 Header gá»‘c trong Fasta: ['>sp|A0A0C5B5G6|MOTSC_HUMAN Mitochondrial-derived peptide MOTS-c OS=Homo sapiens OX=9606 GN=MT-RNR1 PE=1 SV=1', '>sp|A0JNW5|BLT3B_HUMAN Bridge-like lipid transfer protein family member 3B OS=Homo sapiens OX=9606 GN=BLTP3B PE=1 SV=2', '>sp|A0JP26|POTB3_HUMAN POTE ankyrin domain family member B3 OS=Homo sapiens OX=9606 GN=POTEB3 PE=1 SV=2', '>sp|A0PK11|CLRN2_HUMAN Clarin-2 OS=Homo sapiens OX=9606 GN=CLRN2 PE=1 SV=1', '>sp|A1A4S6|RHG10_HUMAN Rho GTPase-activating protein 10 OS=Homo sapiens OX=9606 GN=ARHGAP10 PE=1 SV=1']\n",
      "   + 5 ID sau khi lÃ m sáº¡ch   : ['A0A0C5B5G6', 'A0JNW5', 'A0JP26', 'A0PK11', 'A1A4S6']\n",
      "-> Äang Ä‘á»c file nhÃ£n...\n",
      "   + 5 ID gá»‘c trong file Terms: ['Q5W0B1', 'Q5W0B1', 'Q5W0B1', 'Q5W0B1', 'Q5W0B1']\n",
      "   + 5 ID Terms sau khi clean : ['Q5W0B1', 'Q5W0B1', 'Q5W0B1', 'Q5W0B1', 'Q5W0B1']\n",
      "\n",
      "---> Káº¾T QUáº¢ KHá»šP: TÃ¬m tháº¥y 82404 ID chung.\n",
      "\n",
      "âœ… ÄÃ£ khá»›p ID! Äang tiáº¿n hÃ nh huáº¥n luyá»‡n...\n",
      "-> Train shape: (82404, 20) | Y shape: (82404, 300)\n",
      "-> Training xong!\n",
      "[3/4] Dá»± Ä‘oÃ¡n Test...\n",
      "-> Äang Ä‘á»c file: testsuperset.fasta...\n",
      "[4/4] Ghi file submission...\n",
      "âœ… HOÃ€N Táº¤T!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import gc\n",
    "\n",
    "print(\"--- ðŸ› ï¸ CHáº¾ Äá»˜ Sá»¬A Lá»–I KHá»šP ID (DEBUG & FIX) ---\")\n",
    "\n",
    "# --- Cáº¤U HÃŒNH ---\n",
    "TRAIN_SEQ = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta'\n",
    "TRAIN_TERMS = '/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv'\n",
    "TEST_SEQ = '/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta'\n",
    "\n",
    "NUM_TERMS = 300\n",
    "AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "# --- 1. HÃ€M LÃ€M Sáº CH ID (Máº NH HÆ N) ---\n",
    "def advanced_clean_id(raw_id):\n",
    "    # XÃ³a dáº¥u > vÃ  khoáº£ng tráº¯ng thá»«a\n",
    "    raw_id = raw_id.strip().replace('>', '')\n",
    "    \n",
    "    # TrÆ°á»ng há»£p 1: Chuá»—i cÃ³ dáº¥u gáº¡ch Ä‘á»©ng |\n",
    "    if '|' in raw_id:\n",
    "        parts = raw_id.split('|')\n",
    "        # ThÆ°á»ng ID chuáº©n náº±m á»Ÿ giá»¯a (index 1)\n",
    "        if len(parts) >= 2:\n",
    "            return parts[1]\n",
    "            \n",
    "    # TrÆ°á»ng há»£p 2: Chuá»—i cÃ³ khoáº£ng tráº¯ng \n",
    "    if ' ' in raw_id:\n",
    "        return raw_id.split(' ')[0]\n",
    "        \n",
    "    # TrÆ°á»ng há»£p máº·c Ä‘á»‹nh: Giá»¯ nguyÃªn\n",
    "    return raw_id\n",
    "\n",
    "# --- 2. HÃ€M Táº O Äáº¶C TRÆ¯NG ---\n",
    "def get_aac(sequence):\n",
    "    seq = sequence.upper()\n",
    "    length = len(seq)\n",
    "    if length == 0: return [0.0]*20\n",
    "    return [seq.count(aa) / length for aa in AMINO_ACIDS]\n",
    "\n",
    "def load_data_robust(fasta_path):\n",
    "    print(f\"-> Äang Ä‘á»c file: {os.path.basename(fasta_path)}...\")\n",
    "    ids = []\n",
    "    features = []\n",
    "    raw_headers = [] # LÆ°u láº¡i Ä‘á»ƒ debug\n",
    "    \n",
    "    with open(fasta_path, 'r') as f:\n",
    "        cid, cseq = \"\", []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if cid:\n",
    "                    ids.append(cid); features.append(get_aac(\"\".join(cseq)))\n",
    "                \n",
    "                # LÆ°u header gá»‘c Ä‘á»ƒ kiá»ƒm tra\n",
    "                if len(raw_headers) < 5: raw_headers.append(line)\n",
    "                \n",
    "                # Ãp dá»¥ng lÃ m sáº¡ch ID\n",
    "                cid = advanced_clean_id(line)\n",
    "                cseq = []\n",
    "            else: cseq.append(line)\n",
    "        if cid: ids.append(cid); features.append(get_aac(\"\".join(cseq)))\n",
    "            \n",
    "    return ids, np.array(features, dtype=np.float32), raw_headers\n",
    "\n",
    "# --- 3. KIá»‚M TRA Dá»® LIá»†U (QUAN TRá»ŒNG) ---\n",
    "print(\"[1/4] Äá»c vÃ  Soi dá»¯ liá»‡u...\")\n",
    "\n",
    "train_ids, X_train, sample_headers = load_data_robust(TRAIN_SEQ)\n",
    "print(f\"   + 5 Header gá»‘c trong Fasta: {sample_headers}\")\n",
    "print(f\"   + 5 ID sau khi lÃ m sáº¡ch   : {train_ids[:5]}\")\n",
    "\n",
    "print(\"-> Äang Ä‘á»c file nhÃ£n...\")\n",
    "train_terms = pd.read_csv(TRAIN_TERMS, sep='\\t', header=None, skiprows=1, usecols=[0, 1])\n",
    "# Äáº·t tÃªn cá»™t táº¡m\n",
    "train_terms.columns = [\"EntryID\", \"term\"]\n",
    "\n",
    "# In máº«u ID trong file Terms\n",
    "print(f\"   + 5 ID gá»‘c trong file Terms: {train_terms['EntryID'].head(5).tolist()}\")\n",
    "\n",
    "# Thá»­ lÃ m sáº¡ch ID trong file Terms luÃ´n \n",
    "train_terms['EntryID_Clean'] = train_terms['EntryID'].apply(lambda x: advanced_clean_id(str(x)))\n",
    "print(f\"   + 5 ID Terms sau khi clean : {train_terms['EntryID_Clean'].head(5).tolist()}\")\n",
    "\n",
    "# --- 4. KHá»šP Dá»® LIá»†U ---\n",
    "train_ids_set = set(train_ids)\n",
    "terms_ids_set = set(train_terms['EntryID_Clean'].unique())\n",
    "common_ids = train_ids_set.intersection(terms_ids_set)\n",
    "\n",
    "print(f\"\\n---> Káº¾T QUáº¢ KHá»šP: TÃ¬m tháº¥y {len(common_ids)} ID chung.\")\n",
    "\n",
    "if len(common_ids) == 0:\n",
    "    print(\"\\nâŒ VáºªN KHÃ”NG KHá»šP! HÃ£y nhÃ¬n ká»¹ 2 dÃ²ng 'ID sau khi lÃ m sáº¡ch' á»Ÿ trÃªn xem chÃºng khÃ¡c nhau chá»— nÃ o.\")\n",
    "    print(\"VÃ­ dá»¥: Má»™t bÃªn lÃ  'P12345', má»™t bÃªn lÃ  'P12345_HUMAN' -> KhÃ¡c nhau.\")\n",
    "    raise ValueError(\"Dá»«ng chÆ°Æ¡ng trÃ¬nh Ä‘á»ƒ kiá»ƒm tra ID.\")\n",
    "\n",
    "# --- 5. Náº¾U KHá»šP THÃŒ CHáº Y TIáº¾P ---\n",
    "print(\"\\nâœ… ÄÃ£ khá»›p ID! Äang tiáº¿n hÃ nh huáº¥n luyá»‡n...\")\n",
    "\n",
    "# Lá»c file terms\n",
    "train_terms = train_terms[train_terms['EntryID_Clean'].isin(common_ids)]\n",
    "\n",
    "# Láº¥y Top N\n",
    "top_terms = train_terms['term'].value_counts().head(NUM_TERMS).index.tolist()\n",
    "term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "\n",
    "# Táº¡o Y\n",
    "Y_train = np.zeros((len(train_ids), NUM_TERMS), dtype=np.int8)\n",
    "# Map vá»‹ trÃ­ ID trong train_ids\n",
    "id_to_index = {pid: i for i, pid in enumerate(train_ids) if pid in common_ids}\n",
    "\n",
    "grouped = train_terms.groupby('EntryID_Clean')['term']\n",
    "for pid, terms in grouped:\n",
    "    if pid in id_to_index:\n",
    "        idx_row = id_to_index[pid]\n",
    "        indices = [term_to_idx[t] for t in terms if t in term_to_idx]\n",
    "        Y_train[idx_row, indices] = 1\n",
    "\n",
    "# Loáº¡i bá» cá»™t rá»—ng\n",
    "col_sums = Y_train.sum(axis=0)\n",
    "valid_cols = np.where(col_sums > 0)[0]\n",
    "Y_train = Y_train[:, valid_cols]\n",
    "top_terms = [top_terms[i] for i in valid_cols]\n",
    "NUM_TERMS = len(top_terms)\n",
    "\n",
    "print(f\"-> Train shape: {X_train.shape} | Y shape: {Y_train.shape}\")\n",
    "\n",
    "# Train\n",
    "clf = MultiOutputClassifier(SGDClassifier(loss='log_loss', alpha=0.001, n_jobs=-1, random_state=42))\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"-> Training xong!\")\n",
    "\n",
    "# Predict Test\n",
    "print(\"[3/4] Dá»± Ä‘oÃ¡n Test...\")\n",
    "test_ids, X_test, _ = load_data_robust(TEST_SEQ)\n",
    "\n",
    "if len(test_ids) == 0:\n",
    "    # Fallback tÃ¬m file\n",
    "    for root, _, files in os.walk('/kaggle/input'):\n",
    "        for f in files:\n",
    "            if 'test' in root.lower() and f.endswith('fasta'):\n",
    "                test_ids, X_test, _ = load_data_robust(os.path.join(root, f))\n",
    "                break\n",
    "\n",
    "# Predict tá»«ng pháº§n\n",
    "batch_size = 5000\n",
    "preds_matrix = np.zeros((len(test_ids), NUM_TERMS), dtype=np.float32)\n",
    "for i in range(0, len(test_ids), batch_size):\n",
    "    end = min(i+batch_size, len(test_ids))\n",
    "    p = clf.predict_proba(X_test[i:end])\n",
    "    preds_matrix[i:end] = np.array([prob[:, 1] for prob in p]).T\n",
    "\n",
    "# Ghi file\n",
    "print(\"[4/4] Ghi file submission...\")\n",
    "data = []\n",
    "TOP_K = 30\n",
    "for i, pid in enumerate(test_ids):\n",
    "    sc = preds_matrix[i]\n",
    "    top_ix = np.argsort(sc)[-TOP_K:][::-1]\n",
    "    for idx in top_ix:\n",
    "        if sc[idx] > 0.001:\n",
    "            data.append(f\"{pid}\\t{top_terms[idx]}\\t{sc[idx]:.4f}\\n\")\n",
    "\n",
    "# Fallback\n",
    "if len(data) < len(test_ids):\n",
    "    print(\"-> Adding fallback...\")\n",
    "    naive = pd.Series(col_sums, index=top_terms).nlargest(5) / len(Y_train)\n",
    "    exist = set([x.split('\\t')[0] for x in data])\n",
    "    for pid in test_ids:\n",
    "        if pid not in exist:\n",
    "            for t, s in naive.items():\n",
    "                data.append(f\"{pid}\\t{t}\\t{s:.4f}\\n\")\n",
    "\n",
    "with open('submission.tsv', 'w') as f:\n",
    "    f.writelines(data)\n",
    "    \n",
    "print(\"âœ… HOÃ€N Táº¤T!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 69.380419,
   "end_time": "2025-12-02T01:16:31.940787",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-02T01:15:22.560368",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

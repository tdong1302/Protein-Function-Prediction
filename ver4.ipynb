{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdb94e9",
   "metadata": {
    "papermill": {
     "duration": 0.007196,
     "end_time": "2025-12-03T01:57:08.585246",
     "exception": false,
     "start_time": "2025-12-03T01:57:08.578050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üì¶ B∆Ø·ªöC 0: C√†i ƒê·∫∑t v√† Import Th∆∞ Vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4c64d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T01:57:08.598581Z",
     "iopub.status.busy": "2025-12-03T01:57:08.598144Z",
     "iopub.status.idle": "2025-12-03T01:57:28.567967Z",
     "shell.execute_reply": "2025-12-03T01:57:28.566748Z"
    },
    "papermill": {
     "duration": 19.978749,
     "end_time": "2025-12-03T01:57:28.569698",
     "exception": false,
     "start_time": "2025-12-03T01:57:08.590949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• ƒêang c√†i ƒë·∫∑t biopython...\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.2/3.2 MB 41.6 MB/s eta 0:00:00\n",
      "‚úÖ biopython c√†i ƒë·∫∑t xong\n",
      "‚úÖ lightgbm ƒë√£ c√†i ƒë·∫∑t\n",
      "üì• ƒêang c√†i ƒë·∫∑t scikit-learn...\n",
      "‚úÖ scikit-learn c√†i ƒë·∫∑t xong\n",
      "‚úÖ pandas ƒë√£ c√†i ƒë·∫∑t\n",
      "‚úÖ numpy ƒë√£ c√†i ƒë·∫∑t\n",
      "‚úÖ tqdm ƒë√£ c√†i ƒë·∫∑t\n",
      "\n",
      "‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán s·∫µn s√†ng!\n"
     ]
    }
   ],
   "source": [
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán \n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_needed(package):\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f'‚úÖ {package} ƒë√£ c√†i ƒë·∫∑t')\n",
    "    except ImportError:\n",
    "        print(f'üì• ƒêang c√†i ƒë·∫∑t {package}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
    "        print(f'‚úÖ {package} c√†i ƒë·∫∑t xong')\n",
    "\n",
    "# C√†i ƒë·∫∑t c√°c package \n",
    "packages = ['biopython', 'lightgbm', 'scikit-learn', 'pandas', 'numpy', 'tqdm']\n",
    "for pkg in packages:\n",
    "    install_if_needed(pkg)\n",
    "\n",
    "print('\\n‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán s·∫µn s√†ng!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f6eff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T01:57:28.584305Z",
     "iopub.status.busy": "2025-12-03T01:57:28.583290Z",
     "iopub.status.idle": "2025-12-03T01:57:33.589880Z",
     "shell.execute_reply": "2025-12-03T01:57:33.588716Z"
    },
    "papermill": {
     "duration": 5.015356,
     "end_time": "2025-12-03T01:57:33.591325",
     "exception": false,
     "start_time": "2025-12-03T01:57:28.575969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import!\n",
      "üìä NumPy: 1.26.4 | Pandas: 2.2.3\n",
      "üî• PyTorch: 2.6.0+cu124 | Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import th∆∞ vi·ªán\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# BioPython\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c import!')\n",
    "print(f'üìä NumPy: {np.__version__} | Pandas: {pd.__version__}')\n",
    "print(f'üî• PyTorch: {torch.__version__} | Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b304f8a",
   "metadata": {
    "papermill": {
     "duration": 0.006734,
     "end_time": "2025-12-03T01:57:33.604561",
     "exception": false,
     "start_time": "2025-12-03T01:57:33.597827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üìÅ B∆Ø·ªöC 1: Thi·∫øt L·∫≠p ƒê∆∞·ªùng D·∫´n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119661e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T01:57:33.619681Z",
     "iopub.status.busy": "2025-12-03T01:57:33.618938Z",
     "iopub.status.idle": "2025-12-03T01:57:33.650767Z",
     "shell.execute_reply": "2025-12-03T01:57:33.649303Z"
    },
    "papermill": {
     "duration": 0.041988,
     "end_time": "2025-12-03T01:57:33.652636",
     "exception": false,
     "start_time": "2025-12-03T01:57:33.610648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ƒêang ch·∫°y tr√™n Kaggle\n",
      "   Available inputs: ['cafa-6']\n",
      "   Dataset dir: /kaggle/input/cafa-6\n",
      "   Contents: ['sample_submission.tsv', 'IA.tsv', 'Test', 'Train']\n",
      "\n",
      "üìÅ Train: /kaggle/input/cafa-6/Train\n",
      "üìÅ Test: /kaggle/input/cafa-6/Test\n",
      "\n",
      "üìã File verification:\n",
      "   train_terms.tsv: ‚úÖ /kaggle/input/cafa-6/Train/train_terms.tsv\n",
      "   train_sequences.fasta: ‚úÖ /kaggle/input/cafa-6/Train/train_sequences.fasta\n",
      "\n",
      "‚úÖ ƒê∆∞·ªùng d·∫´n ƒë√£ thi·∫øt l·∫≠p!\n"
     ]
    }
   ],
   "source": [
    "# Ph√°t hi·ªán m√¥i tr∆∞·ªùng\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    print('üîç ƒêang ch·∫°y tr√™n Kaggle')\n",
    "    \n",
    "    # List all available datasets\n",
    "    input_base = Path('/kaggle/input')\n",
    "    print(f'   Available inputs: {os.listdir(input_base)}')\n",
    "    \n",
    "    # Find CAFA dataset\n",
    "    cafa_dir = None\n",
    "    for item in os.listdir(input_base):\n",
    "        if 'cafa' in item.lower():\n",
    "            cafa_dir = input_base / item\n",
    "            break\n",
    "    \n",
    "    if cafa_dir is None:\n",
    "        cafa_dir = input_base / os.listdir(input_base)[0]\n",
    "    \n",
    "    print(f'   Dataset dir: {cafa_dir}')\n",
    "    print(f'   Contents: {os.listdir(cafa_dir)}')\n",
    "    \n",
    "    # Check structure \n",
    "    if (cafa_dir / 'Train').exists():\n",
    "        TRAIN_PATH = cafa_dir / 'Train'\n",
    "        TEST_PATH = cafa_dir / 'Test'\n",
    "    else:\n",
    "        # Files directly in cafa_dir\n",
    "        TRAIN_PATH = cafa_dir\n",
    "        TEST_PATH = cafa_dir\n",
    "    \n",
    "    IA_PATH = cafa_dir / 'IA.tsv'\n",
    "    OUTPUT_PATH = Path('/kaggle/working')\n",
    "    DATA_PATH = OUTPUT_PATH / 'data'\n",
    "    MODEL_PATH = OUTPUT_PATH / 'models'\n",
    "    SUBMISSION_PATH = OUTPUT_PATH  \n",
    "    \n",
    "else:\n",
    "    print('üîç ƒêang ch·∫°y Local')\n",
    "    TRAIN_PATH = Path('../Train')\n",
    "    TEST_PATH = Path('../Test')\n",
    "    IA_PATH = Path('../IA.tsv')\n",
    "    \n",
    "    DATA_PATH = Path('../data/processed')\n",
    "    MODEL_PATH = Path('../models')\n",
    "    SUBMISSION_PATH = Path('../submissions')\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c output\n",
    "for path in [DATA_PATH, MODEL_PATH, SUBMISSION_PATH]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'\\nüìÅ Train: {TRAIN_PATH}')\n",
    "print(f'üìÅ Test: {TEST_PATH}')\n",
    "\n",
    "# Verify key files exist\n",
    "print(f'\\nüìã File verification:')\n",
    "for fname in ['train_terms.tsv', 'train_sequences.fasta']:\n",
    "    fpath = TRAIN_PATH / fname\n",
    "    exists = fpath.exists()\n",
    "    print(f'   {fname}: {\"‚úÖ\" if exists else \"‚ùå\"} {fpath}')\n",
    "    \n",
    "    if not exists:\n",
    "        # Search for file\n",
    "        print(f'   üîç Searching for {fname}...')\n",
    "        for root, dirs, files in os.walk('/kaggle/input'):\n",
    "            if fname in files:\n",
    "                print(f'      Found: {root}/{fname}')\n",
    "                if 'train_terms' in fname:\n",
    "                    TRAIN_PATH = Path(root)\n",
    "\n",
    "print('\\n‚úÖ ƒê∆∞·ªùng d·∫´n ƒë√£ thi·∫øt l·∫≠p!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e991dd",
   "metadata": {
    "papermill": {
     "duration": 0.006046,
     "end_time": "2025-12-03T01:57:33.665097",
     "exception": false,
     "start_time": "2025-12-03T01:57:33.659051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üì• B∆Ø·ªöC 2: T·∫£i D·ªØ Li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e82cf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T01:57:33.678975Z",
     "iopub.status.busy": "2025-12-03T01:57:33.678645Z",
     "iopub.status.idle": "2025-12-03T01:57:37.290097Z",
     "shell.execute_reply": "2025-12-03T01:57:37.288901Z"
    },
    "papermill": {
     "duration": 3.620892,
     "end_time": "2025-12-03T01:57:37.291900",
     "exception": false,
     "start_time": "2025-12-03T01:57:33.671008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìÇ LOADING DATA\n",
      "============================================================\n",
      "üìã Loading train_terms.tsv...\n",
      "   ‚úÖ 537,027 annotations\n",
      "   Columns: ['EntryID', 'term', 'aspect']\n",
      "   üìù Renamed columns: {'EntryID': 'protein_id', 'term': 'go_term'}\n",
      "   New columns: ['protein_id', 'go_term', 'aspect']\n",
      "\n",
      "üß¨ Loading train_sequences.fasta...\n",
      "   ‚úÖ 82,404 sequences\n",
      "   Sample FASTA IDs: ['A0A0C5B5G6', 'A0JNW5', 'A0JP26']\n",
      "   Sample train_terms IDs: ['Q5W0B1', 'Q5W0B1', 'Q5W0B1']\n",
      "\n",
      "üß™ Loading testsuperset.fasta...\n",
      "   ‚úÖ 224,309 test sequences\n",
      "\n",
      "‚öñÔ∏è Loading IA.tsv...\n",
      "   ‚úÖ 40,122 GO term weights\n",
      "\n",
      "üîç ID Matching Check:\n",
      "   train_terms IDs: 82,404\n",
      "   train_sequences IDs: 82,404\n",
      "   Overlapping IDs: 82,404\n",
      "   ‚úÖ All proteins have sequences!\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA LOADED SUCCESSFULLY!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('üìÇ LOADING DATA')\n",
    "print('='*60)\n",
    "\n",
    "def extract_protein_id(fasta_id):\n",
    "    \"\"\"\n",
    "    Extract UniProt accession t·ª´ FASTA ID\n",
    "    Handles multiple formats:\n",
    "    - \"sp|A0A0C5B5G6|MOTSC_HUMAN\" -> \"A0A0C5B5G6\"\n",
    "    - \"A0A0C5B5G6 9606\" -> \"A0A0C5B5G6\"\n",
    "    - \"A0A0C5B5G6\" -> \"A0A0C5B5G6\"\n",
    "    \"\"\"\n",
    "    fasta_id = fasta_id.split()[0]\n",
    "    \n",
    "    if '|' in fasta_id:\n",
    "        parts = fasta_id.split('|')\n",
    "        return parts[1] if len(parts) >= 2 else fasta_id\n",
    "    \n",
    "    return fasta_id\n",
    "\n",
    "# 1. Load annotations (train_terms.tsv)\n",
    "print('üìã Loading train_terms.tsv...')\n",
    "train_terms = pd.read_csv(TRAIN_PATH / 'train_terms.tsv', sep='\\t')\n",
    "print(f'   ‚úÖ {len(train_terms):,} annotations')\n",
    "print(f'   Columns: {list(train_terms.columns)}')\n",
    "\n",
    "# Standardize column names\n",
    "column_mapping = {}\n",
    "if 'EntryID' in train_terms.columns:\n",
    "    column_mapping['EntryID'] = 'protein_id'\n",
    "if 'term' in train_terms.columns:\n",
    "    column_mapping['term'] = 'go_term'\n",
    "\n",
    "if column_mapping:\n",
    "    train_terms = train_terms.rename(columns=column_mapping)\n",
    "    print(f'   üìù Renamed columns: {column_mapping}')\n",
    "    print(f'   New columns: {list(train_terms.columns)}')\n",
    "\n",
    "# 2. Load protein sequences\n",
    "print('\\nüß¨ Loading train_sequences.fasta...')\n",
    "train_sequences = {}\n",
    "for record in SeqIO.parse(TRAIN_PATH / 'train_sequences.fasta', 'fasta'):\n",
    "    accession = extract_protein_id(record.id)\n",
    "    train_sequences[accession] = str(record.seq)\n",
    "print(f'   ‚úÖ {len(train_sequences):,} sequences')\n",
    "\n",
    "# Debug: Show first few IDs\n",
    "print(f'   Sample FASTA IDs: {list(train_sequences.keys())[:3]}')\n",
    "print(f'   Sample train_terms IDs: {train_terms[\"protein_id\"].head(3).tolist()}')\n",
    "\n",
    "# 3. Load test sequences\n",
    "print('\\nüß™ Loading testsuperset.fasta...')\n",
    "test_sequences = {}\n",
    "for record in SeqIO.parse(TEST_PATH / 'testsuperset.fasta', 'fasta'):\n",
    "    accession = extract_protein_id(record.id)\n",
    "    test_sequences[accession] = str(record.seq)\n",
    "print(f'   ‚úÖ {len(test_sequences):,} test sequences')\n",
    "\n",
    "# 4. Load IA weights \n",
    "print('\\n‚öñÔ∏è Loading IA.tsv...')\n",
    "try:\n",
    "    ia_weights = pd.read_csv(IA_PATH, sep='\\t', header=None, names=['go_term', 'ia_weight'])\n",
    "    print(f'   ‚úÖ {len(ia_weights):,} GO term weights')\n",
    "except:\n",
    "    ia_weights = None\n",
    "    print('   ‚ö†Ô∏è IA.tsv not found')\n",
    "\n",
    "# Verify overlap\n",
    "train_ids = set(train_terms['protein_id'].unique())\n",
    "seq_ids = set(train_sequences.keys())\n",
    "overlap = train_ids & seq_ids\n",
    "print(f'\\nüîç ID Matching Check:')\n",
    "print(f'   train_terms IDs: {len(train_ids):,}')\n",
    "print(f'   train_sequences IDs: {len(seq_ids):,}')\n",
    "print(f'   Overlapping IDs: {len(overlap):,}')\n",
    "\n",
    "if len(overlap) == 0:\n",
    "    print('   ‚ö†Ô∏è WARNING: No matching IDs! Check ID format.')\n",
    "    print(f'\\n   Debug - train_terms samples: {list(train_ids)[:3]}')\n",
    "    print(f'   Debug - sequence samples: {list(seq_ids)[:3]}')\n",
    "elif len(overlap) < len(train_ids):\n",
    "    print(f'   ‚ö†Ô∏è Missing sequences for {len(train_ids) - len(overlap):,} proteins')\n",
    "else:\n",
    "    print('   ‚úÖ All proteins have sequences!')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ DATA LOADED SUCCESSFULLY!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff723fbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T01:57:37.308459Z",
     "iopub.status.busy": "2025-12-03T01:57:37.308112Z",
     "iopub.status.idle": "2025-12-03T01:57:38.563362Z",
     "shell.execute_reply": "2025-12-03T01:57:38.562315Z"
    },
    "papermill": {
     "duration": 1.26547,
     "end_time": "2025-12-03T01:57:38.565478",
     "exception": false,
     "start_time": "2025-12-03T01:57:37.300008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üå≥ LOADING GO HIERARCHY\n",
      "============================================================\n",
      "üìÇ Loading: /kaggle/input/cafa-6/Train/go-basic.obo\n",
      "   ‚úÖ GO terms: 40,122\n",
      "   ‚úÖ Obsolete terms: 7,979\n",
      "   üìä By namespace:\n",
      "      ‚Ä¢ biological_process: 25,950\n",
      "      ‚Ä¢ molecular_function: 10,131\n",
      "      ‚Ä¢ cellular_component: 4,040\n",
      "      ‚Ä¢ external: 1\n",
      "\n",
      "üîó Building ancestor cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building GO hierarchy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26125/26125 [00:00<00:00, 64010.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Average ancestors per term: 13.3\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# LOAD GO HIERARCHY (go-basic.obo)\n",
    "print('='*60)\n",
    "print('üå≥ LOADING GO HIERARCHY')\n",
    "print('='*60)\n",
    "\n",
    "def parse_obo_file(obo_path):\n",
    "    \"\"\"Parse GO OBO file to extract hierarchy\"\"\"\n",
    "    go_parents = {}  \n",
    "    go_names = {}   \n",
    "    go_namespace = {}  \n",
    "    obsolete_terms = set()\n",
    "    \n",
    "    current_term = None\n",
    "    current_name = None\n",
    "    current_namespace = None\n",
    "    current_parents = []\n",
    "    is_obsolete = False\n",
    "    \n",
    "    with open(obo_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line == '[Term]':\n",
    "                # Save previous term\n",
    "                if current_term and not is_obsolete:\n",
    "                    go_parents[current_term] = current_parents\n",
    "                    go_names[current_term] = current_name\n",
    "                    go_namespace[current_term] = current_namespace\n",
    "                elif current_term and is_obsolete:\n",
    "                    obsolete_terms.add(current_term)\n",
    "                \n",
    "                # Reset\n",
    "                current_term = None\n",
    "                current_name = None\n",
    "                current_namespace = None\n",
    "                current_parents = []\n",
    "                is_obsolete = False\n",
    "                \n",
    "            elif line.startswith('id: GO:'):\n",
    "                current_term = line.split('id: ')[1].strip()\n",
    "                \n",
    "            elif line.startswith('name: '):\n",
    "                current_name = line.split('name: ')[1].strip()\n",
    "                \n",
    "            elif line.startswith('namespace: '):\n",
    "                current_namespace = line.split('namespace: ')[1].strip()\n",
    "                \n",
    "            elif line.startswith('is_a: GO:'):\n",
    "                parent = line.split('is_a: ')[1].split(' !')[0].strip()\n",
    "                current_parents.append(parent)\n",
    "                \n",
    "            elif line.startswith('relationship: part_of GO:'):\n",
    "                parent = line.split('part_of ')[1].split(' !')[0].strip()\n",
    "                current_parents.append(parent)\n",
    "                \n",
    "            elif line == 'is_obsolete: true':\n",
    "                is_obsolete = True\n",
    "        \n",
    "        # Save last term\n",
    "        if current_term and not is_obsolete:\n",
    "            go_parents[current_term] = current_parents\n",
    "            go_names[current_term] = current_name\n",
    "            go_namespace[current_term] = current_namespace\n",
    "    \n",
    "    return go_parents, go_names, go_namespace, obsolete_terms\n",
    "\n",
    "def get_all_ancestors(term, go_parents, cache=None):\n",
    "    \"\"\"Get all ancestors of a GO term (including itself)\"\"\"\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    \n",
    "    if term in cache:\n",
    "        return cache[term]\n",
    "    \n",
    "    ancestors = {term}\n",
    "    if term in go_parents:\n",
    "        for parent in go_parents[term]:\n",
    "            ancestors.update(get_all_ancestors(parent, go_parents, cache))\n",
    "    \n",
    "    cache[term] = ancestors\n",
    "    return ancestors\n",
    "\n",
    "def build_ancestor_cache(go_terms, go_parents):\n",
    "    \"\"\"Build ancestor cache for all GO terms\"\"\"\n",
    "    cache = {}\n",
    "    for term in tqdm(go_terms, desc='Building GO hierarchy'):\n",
    "        get_all_ancestors(term, go_parents, cache)\n",
    "    return cache\n",
    "\n",
    "# Load GO hierarchy\n",
    "obo_path = TRAIN_PATH / 'go-basic.obo'\n",
    "if obo_path.exists():\n",
    "    print(f'üìÇ Loading: {obo_path}')\n",
    "    go_parents, go_names, go_namespace, obsolete_terms = parse_obo_file(obo_path)\n",
    "    \n",
    "    print(f'   ‚úÖ GO terms: {len(go_parents):,}')\n",
    "    print(f'   ‚úÖ Obsolete terms: {len(obsolete_terms):,}')\n",
    "    \n",
    "    # Count by namespace\n",
    "    ns_counts = {}\n",
    "    for term, ns in go_namespace.items():\n",
    "        ns_counts[ns] = ns_counts.get(ns, 0) + 1\n",
    "    print(f'   üìä By namespace:')\n",
    "    for ns, count in ns_counts.items():\n",
    "        print(f'      ‚Ä¢ {ns}: {count:,}')\n",
    "    \n",
    "    # Build ancestor cache\n",
    "    print('\\nüîó Building ancestor cache...')\n",
    "    all_training_terms = set(train_terms['go_term'].unique())\n",
    "    ancestor_cache = build_ancestor_cache(all_training_terms, go_parents)\n",
    "    \n",
    "    # Count average ancestors\n",
    "    avg_ancestors = np.mean([len(ancestor_cache.get(t, {t})) for t in all_training_terms])\n",
    "    print(f'   ‚úÖ Average ancestors per term: {avg_ancestors:.1f}')\n",
    "    \n",
    "    GO_HIERARCHY_LOADED = True\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è go-basic.obo not found at {obo_path}')\n",
    "    GO_HIERARCHY_LOADED = False\n",
    "    go_parents = {}\n",
    "    ancestor_cache = {}\n",
    "\n",
    "print('\\n' + '='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c87ee8",
   "metadata": {
    "papermill": {
     "duration": 0.006587,
     "end_time": "2025-12-03T01:57:38.579075",
     "exception": false,
     "start_time": "2025-12-03T01:57:38.572488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üìä B∆Ø·ªöC 3: Ph√¢n T√≠ch D·ªØ Li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8787e1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T01:57:38.594004Z",
     "iopub.status.busy": "2025-12-03T01:57:38.593699Z",
     "iopub.status.idle": "2025-12-03T01:57:38.785024Z",
     "shell.execute_reply": "2025-12-03T01:57:38.783720Z"
    },
    "papermill": {
     "duration": 0.20086,
     "end_time": "2025-12-03T01:57:38.786542",
     "exception": false,
     "start_time": "2025-12-03T01:57:38.585682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä DATA ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üß¨ Proteins: 82,404\n",
      "üìö Unique GO Terms: 26,125\n",
      "üìã Total Annotations: 537,027\n",
      "üìà Avg GO terms per protein: 6.52\n",
      "\n",
      "üìä GO Term Distribution:\n",
      "   ‚Ä¢ Rare (1-10 proteins): 18,868 terms\n",
      "   ‚Ä¢ Medium (11-100): 6,594 terms\n",
      "   ‚Ä¢ Common (>100): 663 terms\n",
      "   ‚Ä¢ Most common: GO:0005515 (33,713 proteins)\n",
      "\n",
      "üßµ Sequence Length Stats:\n",
      "   ‚Ä¢ Mean: 526\n",
      "   ‚Ä¢ Median: 409\n",
      "   ‚Ä¢ Min/Max: 3 / 35213\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('üìä DATA ANALYSIS')\n",
    "print('='*60)\n",
    "\n",
    "# Th·ªëng k√™ c∆° b·∫£n\n",
    "n_proteins = train_terms['protein_id'].nunique()\n",
    "n_go_terms = train_terms['go_term'].nunique()\n",
    "n_annotations = len(train_terms)\n",
    "\n",
    "print(f'\\nüß¨ Proteins: {n_proteins:,}')\n",
    "print(f'üìö Unique GO Terms: {n_go_terms:,}')\n",
    "print(f'üìã Total Annotations: {n_annotations:,}')\n",
    "print(f'üìà Avg GO terms per protein: {n_annotations / n_proteins:.2f}')\n",
    "\n",
    "# Ph√¢n b·ªë GO terms\n",
    "go_counts = train_terms['go_term'].value_counts()\n",
    "print(f'\\nüìä GO Term Distribution:')\n",
    "print(f'   ‚Ä¢ Rare (1-10 proteins): {(go_counts <= 10).sum():,} terms')\n",
    "print(f'   ‚Ä¢ Medium (11-100): {((go_counts > 10) & (go_counts <= 100)).sum():,} terms')\n",
    "print(f'   ‚Ä¢ Common (>100): {(go_counts > 100).sum():,} terms')\n",
    "print(f'   ‚Ä¢ Most common: {go_counts.index[0]} ({go_counts.iloc[0]:,} proteins)')\n",
    "\n",
    "# Sequence lengths\n",
    "if train_sequences:\n",
    "    seq_lengths = [len(seq) for seq in train_sequences.values()]\n",
    "    print(f'\\nüßµ Sequence Length Stats:')\n",
    "    print(f'   ‚Ä¢ Mean: {np.mean(seq_lengths):.0f}')\n",
    "    print(f'   ‚Ä¢ Median: {np.median(seq_lengths):.0f}')\n",
    "    print(f'   ‚Ä¢ Min/Max: {min(seq_lengths)} / {max(seq_lengths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a117c",
   "metadata": {
    "papermill": {
     "duration": 0.006866,
     "end_time": "2025-12-03T01:57:38.800464",
     "exception": false,
     "start_time": "2025-12-03T01:57:38.793598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ‚öôÔ∏è B∆Ø·ªöC 4: Feature Engineering\n",
    "\n",
    "T·∫°o c√°c ƒë·∫∑c tr∆∞ng t·ª´ chu·ªói protein:\n",
    "1. **Amino Acid Composition (AAC)**: T·ª∑ l·ªá 20 amino acids\n",
    "2. **Dipeptide Composition (DPC)**: T·ª∑ l·ªá 400 dipeptides\n",
    "3. **Physicochemical Properties**: C√°c t√≠nh ch·∫•t v·∫≠t l√Ω/h√≥a h·ªçc\n",
    "4. **Sequence Statistics**: Chi·ªÅu d√†i, entropy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338aa704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T01:57:38.816547Z",
     "iopub.status.busy": "2025-12-03T01:57:38.816118Z",
     "iopub.status.idle": "2025-12-03T01:57:38.862704Z",
     "shell.execute_reply": "2025-12-03T01:57:38.861542Z"
    },
    "papermill": {
     "duration": 0.057194,
     "end_time": "2025-12-03T01:57:38.864280",
     "exception": false,
     "start_time": "2025-12-03T01:57:38.807086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚öôÔ∏è FEATURE ENGINEERING (ENHANCED)\n",
      "============================================================\n",
      "\n",
      "üîß Feature vector size: 150\n",
      "   Sample features (first 10): [0.0016     0.28332132 0.016      0.         0.         0.\n",
      " 0.0625     0.0625     0.0625     0.        ]\n",
      "   Non-zero features: 83\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('‚öôÔ∏è FEATURE ENGINEERING (ENHANCED)')\n",
    "print('='*60)\n",
    "\n",
    "# Amino acids\n",
    "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "# Physicochemical properties\n",
    "HYDROPHOBIC = set('AILMFVWP')\n",
    "HYDROPHILIC = set('RKDENQ')\n",
    "POLAR = set('STNQ')\n",
    "CHARGED_POS = set('RKH')\n",
    "CHARGED_NEG = set('DE')\n",
    "AROMATIC = set('FYW')\n",
    "SMALL = set('AGSTC')\n",
    "ALIPHATIC = set('ILV')\n",
    "SULFUR = set('CM')\n",
    "HYDROXYL = set('ST')\n",
    "\n",
    "# Molecular weights\n",
    "AA_MW = {'A': 89, 'C': 121, 'D': 133, 'E': 147, 'F': 165, 'G': 75, 'H': 155, \n",
    "         'I': 131, 'K': 146, 'L': 131, 'M': 149, 'N': 132, 'P': 115, 'Q': 146,\n",
    "         'R': 174, 'S': 105, 'T': 119, 'V': 117, 'W': 204, 'Y': 181}\n",
    "\n",
    "# Hydrophobicity scale (Kyte-Doolittle)\n",
    "HYDRO_SCALE = {'A': 1.8, 'C': 2.5, 'D': -3.5, 'E': -3.5, 'F': 2.8, 'G': -0.4,\n",
    "               'H': -3.2, 'I': 4.5, 'K': -3.9, 'L': 3.8, 'M': 1.9, 'N': -3.5,\n",
    "               'P': -1.6, 'Q': -3.5, 'R': -4.5, 'S': -0.8, 'T': -0.7, 'V': 4.2,\n",
    "               'W': -0.9, 'Y': -1.3}\n",
    "\n",
    "def extract_features_enhanced(sequence):\n",
    "    \"\"\"Extract comprehensive features from protein sequence\"\"\"\n",
    "    seq = ''.join(aa for aa in sequence.upper() if aa in AMINO_ACIDS)\n",
    "    length = len(seq)\n",
    "    \n",
    "    if length == 0:\n",
    "        return np.zeros(150) \n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # 1. Sequence length features (3)\n",
    "    features.append(length / 10000)\n",
    "    features.append(np.log1p(length) / 10)\n",
    "    features.append(min(length, 1000) / 1000)  # Capped length\n",
    "    \n",
    "    # 2. Amino Acid Composition (20)\n",
    "    aa_counts = Counter(seq)\n",
    "    for aa in AMINO_ACIDS:\n",
    "        features.append(aa_counts.get(aa, 0) / length)\n",
    "    \n",
    "    # 3. Physicochemical properties (15)\n",
    "    features.append(sum(1 for aa in seq if aa in HYDROPHOBIC) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in HYDROPHILIC) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in POLAR) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in CHARGED_POS) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in CHARGED_NEG) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in AROMATIC) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in SMALL) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in ALIPHATIC) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in SULFUR) / length)\n",
    "    features.append(sum(1 for aa in seq if aa in HYDROXYL) / length)\n",
    "    \n",
    "    # Net charge\n",
    "    pos_charge = sum(1 for aa in seq if aa in CHARGED_POS)\n",
    "    neg_charge = sum(1 for aa in seq if aa in CHARGED_NEG)\n",
    "    features.append((pos_charge - neg_charge) / length)\n",
    "    \n",
    "    # Molecular weight\n",
    "    mw = sum(AA_MW.get(aa, 110) for aa in seq)\n",
    "    features.append(mw / (length * 200))  \n",
    "    \n",
    "    # Hydrophobicity stats\n",
    "    hydro_values = [HYDRO_SCALE.get(aa, 0) for aa in seq]\n",
    "    features.append(np.mean(hydro_values) / 5)\n",
    "    features.append(np.std(hydro_values) / 5)\n",
    "    features.append(np.max(hydro_values) / 5 if hydro_values else 0)\n",
    "    \n",
    "    # 4. Sequence complexity (3)\n",
    "    aa_freq = np.array([aa_counts.get(aa, 0) / length for aa in AMINO_ACIDS])\n",
    "    aa_freq = aa_freq[aa_freq > 0]\n",
    "    entropy = -np.sum(aa_freq * np.log2(aa_freq)) if len(aa_freq) > 0 else 0\n",
    "    features.append(entropy / 4.5)\n",
    "    \n",
    "    # Unique AA ratio\n",
    "    features.append(len(aa_counts) / 20)\n",
    "    \n",
    "    # Repeat ratio\n",
    "    repeats = sum(1 for i in range(len(seq)-1) if seq[i] == seq[i+1])\n",
    "    features.append(repeats / max(1, length - 1))\n",
    "    \n",
    "    # 5. N-terminal and C-terminal signals (16)\n",
    "    for term_seq, name in [(seq[:50], 'N'), (seq[-50:], 'C')]:\n",
    "        term_len = len(term_seq)\n",
    "        if term_len > 0:\n",
    "            features.append(sum(1 for aa in term_seq if aa in HYDROPHOBIC) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in CHARGED_POS) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in CHARGED_NEG) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in POLAR) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in AROMATIC) / term_len)\n",
    "            features.append(sum(1 for aa in term_seq if aa in SMALL) / term_len)\n",
    "            # Hydrophobicity\n",
    "            term_hydro = [HYDRO_SCALE.get(aa, 0) for aa in term_seq]\n",
    "            features.append(np.mean(term_hydro) / 5)\n",
    "            features.append(np.std(term_hydro) / 5 if len(term_hydro) > 1 else 0)\n",
    "        else:\n",
    "            features.extend([0] * 8)\n",
    "    \n",
    "    # 6. Dipeptide features (30 most informative)\n",
    "    important_dipeptides = [\n",
    "        'LL', 'AA', 'VV', 'GG', 'SS', 'EE', 'KK', 'AL', 'LA', 'EK',\n",
    "        'KE', 'DE', 'ED', 'RK', 'LE', 'EL', 'AE', 'EA', 'LK', 'KL',\n",
    "        'GS', 'SG', 'PP', 'PG', 'GP', 'VL', 'LV', 'IL', 'LI', 'TT'\n",
    "    ]\n",
    "    for dp in important_dipeptides:\n",
    "        count = seq.count(dp)\n",
    "        features.append(count / max(1, length - 1))\n",
    "    \n",
    "    # 7. Tripeptide features (20 key patterns)\n",
    "    important_tripeptides = [\n",
    "        'LLL', 'AAA', 'GGG', 'SSS', 'EEE', 'KKK', 'PPP', 'VVV',\n",
    "        'LEK', 'EKL', 'ALA', 'GAS', 'VAL', 'ILE', 'PRO', 'GLY',\n",
    "        'SER', 'THR', 'ASP', 'GLU'\n",
    "    ]\n",
    "    for tp in important_tripeptides:\n",
    "        count = seq.count(tp)\n",
    "        features.append(count / max(1, length - 2))\n",
    "    \n",
    "    # 8. Position-specific features (20)\n",
    "    # Split sequence into 5 regions\n",
    "    region_size = length // 5 if length >= 5 else length\n",
    "    for i in range(5):\n",
    "        start = i * region_size\n",
    "        end = start + region_size if i < 4 else length\n",
    "        region = seq[start:end]\n",
    "        if len(region) > 0:\n",
    "            features.append(sum(1 for aa in region if aa in HYDROPHOBIC) / len(region))\n",
    "            features.append(sum(1 for aa in region if aa in CHARGED_POS) / len(region))\n",
    "            features.append(sum(1 for aa in region if aa in CHARGED_NEG) / len(region))\n",
    "            features.append(sum(1 for aa in region if aa in AROMATIC) / len(region))\n",
    "        else:\n",
    "            features.extend([0] * 4)\n",
    "    \n",
    "    # 9. Pseudo Amino Acid Composition (23)\n",
    "    # Sequence-order correlation factors\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        if length > lag:\n",
    "            # Hydrophobicity correlation\n",
    "            h_corr = sum(HYDRO_SCALE.get(seq[i], 0) * HYDRO_SCALE.get(seq[i+lag], 0) \n",
    "                        for i in range(length - lag)) / (length - lag)\n",
    "            features.append(h_corr / 25)\n",
    "        else:\n",
    "            features.append(0)\n",
    "    \n",
    "    # Type I PseAAC features\n",
    "    for d in [1, 2, 3]:  \n",
    "        if length > d:\n",
    "            hydro_diff = sum(abs(HYDRO_SCALE.get(seq[i], 0) - HYDRO_SCALE.get(seq[i+d], 0)) \n",
    "                            for i in range(length - d)) / (length - d)\n",
    "            mw_diff = sum(abs(AA_MW.get(seq[i], 110) - AA_MW.get(seq[i+d], 110)) \n",
    "                         for i in range(length - d)) / (length - d)\n",
    "            features.append(hydro_diff / 10)\n",
    "            features.append(mw_diff / 200)\n",
    "            \n",
    "            # Charge transition\n",
    "            charge_trans = sum(1 for i in range(length - d) \n",
    "                              if (seq[i] in CHARGED_POS) != (seq[i+d] in CHARGED_POS)) / (length - d)\n",
    "            features.append(charge_trans)\n",
    "            \n",
    "            # Polarity transition\n",
    "            polar_trans = sum(1 for i in range(length - d)\n",
    "                             if (seq[i] in POLAR) != (seq[i+d] in POLAR)) / (length - d)\n",
    "            features.append(polar_trans)\n",
    "            \n",
    "            # Size transition\n",
    "            size_trans = sum(1 for i in range(length - d)\n",
    "                            if (seq[i] in SMALL) != (seq[i+d] in SMALL)) / (length - d)\n",
    "            features.append(size_trans)\n",
    "            \n",
    "            # Aromatic clustering\n",
    "            arom_cluster = sum(1 for i in range(length - d)\n",
    "                              if seq[i] in AROMATIC and seq[i+d] in AROMATIC) / (length - d)\n",
    "            features.append(arom_cluster)\n",
    "        else:\n",
    "            features.extend([0] * 6)\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# Test feature extraction\n",
    "test_seq = list(train_sequences.values())[0]\n",
    "test_features = extract_features_enhanced(test_seq)\n",
    "print(f'\\nüîß Feature vector size: {len(test_features)}')\n",
    "print(f'   Sample features (first 10): {test_features[:10]}')\n",
    "print(f'   Non-zero features: {np.sum(test_features != 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1fd01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T01:57:38.880208Z",
     "iopub.status.busy": "2025-12-03T01:57:38.879872Z",
     "iopub.status.idle": "2025-12-03T02:02:06.309889Z",
     "shell.execute_reply": "2025-12-03T02:02:06.307891Z"
    },
    "papermill": {
     "duration": 267.440634,
     "end_time": "2025-12-03T02:02:06.312090",
     "exception": false,
     "start_time": "2025-12-03T01:57:38.871456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß¨ Extracting features for training proteins...\n",
      "   Proteins in train_terms: 82,404\n",
      "   Sequences available: 82,404\n",
      "   Proteins with sequences: 82,404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82404/82404 [04:22<00:00, 314.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Features extracted: (82404, 150)\n",
      "   Valid proteins: 82,404 / 82,404\n",
      "\n",
      "üìä Creating label matrix (vectorized)...\n",
      "‚úÖ Label matrix: (82404, 26125)\n",
      "   Positive labels: 537,027 (0.0249%)\n"
     ]
    }
   ],
   "source": [
    "# Extract features for all training proteins\n",
    "print('\\nüß¨ Extracting features for training proteins...')\n",
    "\n",
    "proteins = train_terms['protein_id'].unique()\n",
    "all_go_terms = train_terms['go_term'].unique()\n",
    "\n",
    "print(f'   Proteins in train_terms: {len(proteins):,}')\n",
    "print(f'   Sequences available: {len(train_sequences):,}')\n",
    "\n",
    "# Debug: Check matching\n",
    "matched = sum(1 for p in proteins if p in train_sequences)\n",
    "print(f'   Proteins with sequences: {matched:,}')\n",
    "\n",
    "if matched == 0:\n",
    "    print('\\n‚ùå ERROR: No proteins match! Checking IDs...')\n",
    "    print(f'   First 3 train_terms IDs: {proteins[:3].tolist()}')\n",
    "    print(f'   First 3 sequence IDs: {list(train_sequences.keys())[:3]}')\n",
    "    raise ValueError(\"No matching protein IDs between train_terms and train_sequences!\")\n",
    "\n",
    "# Extract features with ENHANCED function\n",
    "X_features = []\n",
    "valid_proteins = []\n",
    "\n",
    "for pid in tqdm(proteins, desc='Extracting features'):\n",
    "    seq = train_sequences.get(pid, '')\n",
    "    if seq:\n",
    "        X_features.append(extract_features_enhanced(seq))\n",
    "        valid_proteins.append(pid)\n",
    "\n",
    "X = np.array(X_features)\n",
    "print(f'\\n‚úÖ Features extracted: {X.shape}')\n",
    "print(f'   Valid proteins: {len(valid_proteins):,} / {len(proteins):,}')\n",
    "\n",
    "if len(valid_proteins) == 0:\n",
    "    raise ValueError(\"No features extracted! Check protein ID matching.\")\n",
    "\n",
    "# üìä T·∫°o label matrix - VECTORIZED\n",
    "print('\\nüìä Creating label matrix (vectorized)...')\n",
    "\n",
    "protein_to_idx = {p: i for i, p in enumerate(valid_proteins)}\n",
    "term_to_idx = {t: i for i, t in enumerate(all_go_terms)}\n",
    "\n",
    "# Vectorized approach\n",
    "valid_mask = train_terms['protein_id'].isin(protein_to_idx)\n",
    "valid_terms_df = train_terms[valid_mask]\n",
    "\n",
    "row_indices = valid_terms_df['protein_id'].map(protein_to_idx).values\n",
    "col_indices = valid_terms_df['go_term'].map(term_to_idx).values\n",
    "\n",
    "y = np.zeros((len(valid_proteins), len(all_go_terms)), dtype=np.int8)\n",
    "y[row_indices, col_indices] = 1\n",
    "\n",
    "print(f'‚úÖ Label matrix: {y.shape}')\n",
    "print(f'   Positive labels: {y.sum():,} ({100*y.sum()/(y.shape[0]*y.shape[1]):.4f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79fe74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T02:02:06.594618Z",
     "iopub.status.busy": "2025-12-03T02:02:06.594270Z",
     "iopub.status.idle": "2025-12-03T02:02:15.395864Z",
     "shell.execute_reply": "2025-12-03T02:02:15.394492Z"
    },
    "papermill": {
     "duration": 8.926513,
     "end_time": "2025-12-03T02:02:15.397614",
     "exception": false,
     "start_time": "2025-12-03T02:02:06.471101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìê Scaling features...\n",
      "‚úÖ Features scaled: (82404, 150)\n",
      "\n",
      "üìä GO Term Filtering:\n",
      "   ‚Ä¢ Total: 26,125\n",
      "   ‚Ä¢ With ‚â•20 proteins: 4,227\n",
      "‚úÖ Filtered: 4,227 GO terms\n",
      "\n",
      "‚öñÔ∏è Preparing IA weights for weighted loss...\n",
      "   IA weights range: [0.000, 7.435]\n",
      "\n",
      "‚öñÔ∏è Calculating class weights...\n",
      "   Class weights range: [0.50, 0.67]\n"
     ]
    }
   ],
   "source": [
    "#  Scale Features & Filter GO Terms & Create IA Weights\n",
    "print('\\nüìê Scaling features...')\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(f'‚úÖ Features scaled: {X_scaled.shape}')\n",
    "\n",
    "# Filter GO terms\n",
    "MIN_PROTEINS_PER_TERM = 20  \n",
    "\n",
    "term_counts = y.sum(axis=0)\n",
    "valid_mask = term_counts >= MIN_PROTEINS_PER_TERM\n",
    "\n",
    "print(f'\\nüìä GO Term Filtering:')\n",
    "print(f'   ‚Ä¢ Total: {len(term_counts):,}')\n",
    "print(f'   ‚Ä¢ With ‚â•{MIN_PROTEINS_PER_TERM} proteins: {valid_mask.sum():,}')\n",
    "\n",
    "y_filtered = y[:, valid_mask]\n",
    "filtered_go_terms = all_go_terms[valid_mask]\n",
    "\n",
    "print(f'‚úÖ Filtered: {y_filtered.shape[1]:,} GO terms')\n",
    "\n",
    "#  Prepare IA Weights for Loss Function\n",
    "print('\\n‚öñÔ∏è Preparing IA weights for weighted loss...')\n",
    "\n",
    "if ia_weights is not None:\n",
    "    ia_dict = dict(zip(ia_weights['go_term'], ia_weights['ia_weight']))\n",
    "    term_ia_weights = np.array([ia_dict.get(t, 1.0) for t in filtered_go_terms])\n",
    "    # Normalize weights\n",
    "    term_ia_weights = term_ia_weights / term_ia_weights.mean()\n",
    "    print(f'   IA weights range: [{term_ia_weights.min():.3f}, {term_ia_weights.max():.3f}]')\n",
    "else:\n",
    "    term_ia_weights = np.ones(len(filtered_go_terms))\n",
    "    print('   Using uniform weights (IA.tsv not found)')\n",
    "\n",
    "#  Calculate Class Weights for Imbalanced Data\n",
    "print('\\n‚öñÔ∏è Calculating class weights...')\n",
    "pos_counts = y_filtered.sum(axis=0)\n",
    "neg_counts = y_filtered.shape[0] - pos_counts\n",
    "\n",
    "# Effective number of samples weighting\n",
    "beta = 0.9999\n",
    "pos_weights = (1 - beta) / (1 - np.power(beta, pos_counts + 1))\n",
    "neg_weights = (1 - beta) / (1 - np.power(beta, neg_counts + 1))\n",
    "\n",
    "# Combine with IA weights\n",
    "class_weights = (neg_weights / pos_weights) * term_ia_weights\n",
    "class_weights = np.clip(class_weights, 0.5, 50)\n",
    "print(f'   Class weights range: [{class_weights.min():.2f}, {class_weights.max():.2f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb562d0",
   "metadata": {
    "papermill": {
     "duration": 0.122678,
     "end_time": "2025-12-03T02:02:15.639706",
     "exception": false,
     "start_time": "2025-12-03T02:02:15.517028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ü§ñ B∆Ø·ªöC 5: Training Neural Network\n",
    "\n",
    "S·ª≠ d·ª•ng Neural Network v·ªõi Residual Blocks cho multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8014284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T02:02:15.884699Z",
     "iopub.status.busy": "2025-12-03T02:02:15.884352Z",
     "iopub.status.idle": "2025-12-03T02:02:22.278945Z",
     "shell.execute_reply": "2025-12-03T02:02:22.277724Z"
    },
    "papermill": {
     "duration": 6.520608,
     "end_time": "2025-12-03T02:02:22.280602",
     "exception": false,
     "start_time": "2025-12-03T02:02:15.759994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ü§ñ MODEL TRAINING SETUP (IMPROVED FOR HIGHER SCORE)\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è Configuration (Optimized for Score):\n",
      "   ‚Ä¢ Max GO terms: 2000\n",
      "   ‚Ä¢ Hidden dim: 768\n",
      "   ‚Ä¢ Epochs: 40\n",
      "   ‚Ä¢ Batch size: 128\n",
      "\n",
      "üìä Splitting data...\n",
      "   Train: 74,163 samples\n",
      "   Val: 8,241 samples\n",
      "   GO terms: 2,000\n",
      "   Positive labels in train: 328,678\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('ü§ñ MODEL TRAINING SETUP (IMPROVED FOR HIGHER SCORE)')\n",
    "print('='*60)\n",
    "\n",
    "import gc\n",
    "\n",
    "# C·∫§U H√åNH - Optimized for better performance\n",
    "MAX_GO_TERMS = 2000      \n",
    "HIDDEN_DIM = 768         \n",
    "BATCH_SIZE = 128        \n",
    "NUM_EPOCHS = 40          \n",
    "LEARNING_RATE = 3e-4     \n",
    "WEIGHT_DECAY = 0.01      \n",
    "\n",
    "print(f'\\n‚öôÔ∏è Configuration (Optimized for Score):')\n",
    "print(f'   ‚Ä¢ Max GO terms: {MAX_GO_TERMS}')\n",
    "print(f'   ‚Ä¢ Hidden dim: {HIDDEN_DIM}')\n",
    "print(f'   ‚Ä¢ Epochs: {NUM_EPOCHS}')\n",
    "print(f'   ‚Ä¢ Batch size: {BATCH_SIZE}')\n",
    "\n",
    "# Clear unused memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Train-validation split\n",
    "print('\\nüìä Splitting data...')\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y_filtered,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "print(f'   Train: {X_train.shape[0]:,} samples')\n",
    "print(f'   Val: {X_val.shape[0]:,} samples')\n",
    "\n",
    "# Select top GO terms\n",
    "term_counts = y_train.sum(axis=0)\n",
    "top_term_indices = np.argsort(term_counts)[-MAX_GO_TERMS:]\n",
    "y_train = y_train[:, top_term_indices]\n",
    "y_val = y_val[:, top_term_indices]\n",
    "filtered_go_terms_final = filtered_go_terms[top_term_indices]\n",
    "final_class_weights = class_weights[top_term_indices]\n",
    "\n",
    "# Free memory from filtered arrays\n",
    "del y_filtered\n",
    "gc.collect()\n",
    "\n",
    "print(f'   GO terms: {y_train.shape[1]:,}')\n",
    "print(f'   Positive labels in train: {y_train.sum():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87dadb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T02:02:22.523227Z",
     "iopub.status.busy": "2025-12-03T02:02:22.522933Z",
     "iopub.status.idle": "2025-12-03T03:02:36.397683Z",
     "shell.execute_reply": "2025-12-03T03:02:36.396572Z"
    },
    "papermill": {
     "duration": 3614.120451,
     "end_time": "2025-12-03T03:02:36.522197",
     "exception": false,
     "start_time": "2025-12-03T02:02:22.401746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèãÔ∏è Training Neural Network (Deeper Model)...\n",
      "   Using device: cpu\n",
      "   Model parameters: 9,018,800\n",
      "   Epoch  5/40 | Train: 0.0011 | Val: 0.0011 | F1: 0.0044 | LR: 0.000898\n",
      "   Epoch 10/40 | Train: 0.0010 | Val: 0.0010 | F1: 0.0044 | LR: 0.000840\n",
      "   Epoch 15/40 | Train: 0.0009 | Val: 0.0010 | F1: 0.0044 | LR: 0.000708\n",
      "   Epoch 20/40 | Train: 0.0008 | Val: 0.0010 | F1: 0.0046 | LR: 0.000528\n",
      "   Epoch 25/40 | Train: 0.0008 | Val: 0.0010 | F1: 0.0050 | LR: 0.000333\n",
      "   Epoch 30/40 | Train: 0.0007 | Val: 0.0011 | F1: 0.0055 | LR: 0.000161\n",
      "   Epoch 35/40 | Train: 0.0007 | Val: 0.0011 | F1: 0.0058 | LR: 0.000042\n",
      "   Epoch 40/40 | Train: 0.0007 | Val: 0.0011 | F1: 0.0059 | LR: 0.000000\n",
      "\n",
      "‚úÖ Training completed in 3607.5s (60.1 min)\n",
      "   Best Val F1: 0.0059\n"
     ]
    }
   ],
   "source": [
    "#  Neural Network Model (DEEPER & STRONGER)\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
    "    def __init__(self, dim, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, dim // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim // reduction, dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        scale = self.fc(x)\n",
    "        return x * scale\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),  # LayerNorm thay BatchNorm \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "        self.se = SEBlock(dim)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out = self.se(out)\n",
    "        return self.act(out + residual)\n",
    "\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        # More Residual blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(hidden_dim, 0.15),\n",
    "            ResidualBlock(hidden_dim, 0.15),\n",
    "            ResidualBlock(hidden_dim, 0.15),\n",
    "            ResidualBlock(hidden_dim, 0.15),\n",
    "            ResidualBlock(hidden_dim, 0.10),\n",
    "            ResidualBlock(hidden_dim, 0.10),\n",
    "        )\n",
    "        \n",
    "        # Output layers v·ªõi bottleneck\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.LayerNorm(hidden_dim // 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim // 4, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.res_blocks(x)\n",
    "        return self.output(x)\n",
    "\n",
    "#  Asymmetric Loss - Better for Multi-Label\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    \"\"\"Asymmetric Loss for Multi-Label Classification\"\"\"\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Sigmoid\n",
    "        xs_pos = torch.sigmoid(inputs)\n",
    "        xs_neg = 1 - xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "\n",
    "        # Basic BCE\n",
    "        los_pos = targets * torch.log(xs_pos.clamp(min=1e-8))\n",
    "        los_neg = (1 - targets) * torch.log(xs_neg.clamp(min=1e-8))\n",
    "        \n",
    "        # Asymmetric focusing\n",
    "        pt = xs_pos * targets + xs_neg * (1 - targets)\n",
    "        gamma = self.gamma_pos * targets + self.gamma_neg * (1 - targets)\n",
    "        asymmetric_w = (1 - pt).pow(gamma)\n",
    "        \n",
    "        loss = -asymmetric_w * (los_pos + los_neg)\n",
    "        \n",
    "        # Apply pos_weight\n",
    "        if self.pos_weight is not None:\n",
    "            loss = loss * (targets * self.pos_weight + (1 - targets))\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "#  Prepare DataLoaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train), \n",
    "    torch.FloatTensor(y_train)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val), \n",
    "    torch.FloatTensor(y_val)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "#  Training with Label Smoothing & Better Scheduling\n",
    "print('\\nüèãÔ∏è Training Neural Network (Deeper Model)...')\n",
    "print(f'   Using device: {device}')\n",
    "\n",
    "model = ProteinClassifier(X_train.shape[1], HIDDEN_DIM, y_train.shape[1]).to(device)\n",
    "print(f'   Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "# Optimizer v·ªõi weight decay ri√™ng cho bias\n",
    "no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = optim.AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n",
    "\n",
    "# OneCycleLR - often better than cosine\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=LEARNING_RATE * 3,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "# Asymmetric Loss - better for imbalanced multi-label\n",
    "pos_weight_tensor = torch.FloatTensor(final_class_weights).to(device)\n",
    "criterion = AsymmetricLoss(gamma_neg=4, gamma_pos=1, clip=0.05, pos_weight=pos_weight_tensor)\n",
    "\n",
    "start_time = time.time()\n",
    "best_val_loss = float('inf')\n",
    "best_val_f1 = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            val_loss += criterion(outputs, y_batch).item()\n",
    "            all_val_probs.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "    \n",
    "    # Calculate F1 for early stopping\n",
    "    val_probs_epoch = np.vstack(all_val_probs)\n",
    "    val_preds_epoch = (val_probs_epoch >= 0.05).astype(np.int8)\n",
    "    if val_preds_epoch.sum() > 0:\n",
    "        from sklearn.metrics import f1_score\n",
    "        val_f1 = f1_score(y_val, val_preds_epoch, average='micro', zero_division=0)\n",
    "    else:\n",
    "        val_f1 = 0\n",
    "    \n",
    "    avg_train = train_loss / len(train_loader)\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Early stopping based on F1\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_val_loss = avg_val\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "        print(f'   Epoch {epoch+1:2d}/{NUM_EPOCHS} | Train: {avg_train:.4f} | Val: {avg_val:.4f} | F1: {val_f1:.4f} | LR: {current_lr:.6f}')\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'\\n   ‚ö†Ô∏è Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f'\\n‚úÖ Training completed in {training_time:.1f}s ({training_time/60:.1f} min)')\n",
    "print(f'   Best Val F1: {best_val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341ec82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T03:02:36.770696Z",
     "iopub.status.busy": "2025-12-03T03:02:36.770013Z",
     "iopub.status.idle": "2025-12-03T03:05:01.418165Z",
     "shell.execute_reply": "2025-12-03T03:05:01.416377Z"
    },
    "papermill": {
     "duration": 144.773908,
     "end_time": "2025-12-03T03:05:01.420272",
     "exception": false,
     "start_time": "2025-12-03T03:02:36.646364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Evaluating model...\n",
      "\n",
      "üéØ Finding optimal threshold...\n",
      "   Threshold 0.010: F1=0.0044, P=0.0022, R=1.0000, Avg=1984.0\n",
      "   Threshold 0.020: F1=0.0046, P=0.0023, R=0.9996, Avg=1909.3\n",
      "   Threshold 0.030: F1=0.0049, P=0.0025, R=0.9986, Avg=1782.5\n",
      "   Threshold 0.040: F1=0.0053, P=0.0027, R=0.9962, Avg=1627.9\n",
      "   Threshold 0.050: F1=0.0059, P=0.0030, R=0.9920, Avg=1464.6\n",
      "   Threshold 0.060: F1=0.0066, P=0.0033, R=0.9862, Avg=1303.9\n",
      "   Threshold 0.070: F1=0.0074, P=0.0037, R=0.9792, Avg=1152.6\n",
      "   Threshold 0.080: F1=0.0084, P=0.0042, R=0.9715, Avg=1013.6\n",
      "   Threshold 0.090: F1=0.0094, P=0.0047, R=0.9620, Avg=887.6\n",
      "   Threshold 0.100: F1=0.0107, P=0.0054, R=0.9510, Avg=775.4\n",
      "   Threshold 0.120: F1=0.0137, P=0.0069, R=0.9249, Avg=588.0\n",
      "   Threshold 0.150: F1=0.0198, P=0.0100, R=0.8793, Avg=384.4\n",
      "\n",
      "‚úÖ Best threshold: 0.15 (F1=0.0198)\n",
      "   Using threshold for submission: 0.13\n",
      "\n",
      "üìä Final Results (threshold=0.13):\n",
      "   Train Hamming Loss: 0.249635\n",
      "   Val Hamming Loss: 0.253632\n",
      "   Val F1 (micro): 0.0154\n",
      "   Val Predictions: 4,209,907 positives (25.54%)\n",
      "   Unique GO terms predicted: 2,000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model & Find Optimal Threshold\n",
    "print('\\nüìà Evaluating model...')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def predict_batch(X):\n",
    "    \"\"\"Predict probabilities with model\"\"\"\n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# Get predictions\n",
    "val_probs = predict_batch(X_val)\n",
    "train_probs = predict_batch(X_train)\n",
    "\n",
    "#  Find Optimal Threshold - Finer search\n",
    "print('\\nüéØ Finding optimal threshold...')\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0.01\n",
    "results = []\n",
    "\n",
    "# Finer threshold search\n",
    "for threshold in [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.12, 0.15]:\n",
    "    val_preds = (val_probs >= threshold).astype(np.int8)\n",
    "    \n",
    "    if val_preds.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    f1_micro = f1_score(y_val, val_preds, average='micro', zero_division=0)\n",
    "    precision = precision_score(y_val, val_preds, average='micro', zero_division=0)\n",
    "    recall = recall_score(y_val, val_preds, average='micro', zero_division=0)\n",
    "    coverage = val_preds.sum(axis=1).mean()\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'f1': f1_micro,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'coverage': coverage\n",
    "    })\n",
    "    \n",
    "    print(f'   Threshold {threshold:.3f}: F1={f1_micro:.4f}, P={precision:.4f}, R={recall:.4f}, Avg={coverage:.1f}')\n",
    "    \n",
    "    if f1_micro > best_f1:\n",
    "        best_f1 = f1_micro\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Use slightly lower threshold for submissio\n",
    "# Kaggle metric often benefits from more predictions with good precision\n",
    "PROB_THRESHOLD = max(0.03, best_threshold - 0.02)\n",
    "print(f'\\n‚úÖ Best threshold: {best_threshold} (F1={best_f1:.4f})')\n",
    "print(f'   Using threshold for submission: {PROB_THRESHOLD}')\n",
    "\n",
    "# Final evaluation\n",
    "val_preds = (val_probs >= PROB_THRESHOLD).astype(np.int8)\n",
    "train_preds = (train_probs >= PROB_THRESHOLD).astype(np.int8)\n",
    "\n",
    "val_hamming = hamming_loss(y_val, val_preds)\n",
    "train_hamming = hamming_loss(y_train, train_preds)\n",
    "final_f1 = f1_score(y_val, val_preds, average='micro', zero_division=0)\n",
    "\n",
    "print(f'\\nüìä Final Results (threshold={PROB_THRESHOLD}):')\n",
    "print(f'   Train Hamming Loss: {train_hamming:.6f}')\n",
    "print(f'   Val Hamming Loss: {val_hamming:.6f}')\n",
    "print(f'   Val F1 (micro): {final_f1:.4f}')\n",
    "print(f'   Val Predictions: {val_preds.sum():,} positives ({100*val_preds.mean():.2f}%)')\n",
    "print(f'   Unique GO terms predicted: {(val_preds.sum(axis=0) > 0).sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89ef47",
   "metadata": {
    "papermill": {
     "duration": 0.124501,
     "end_time": "2025-12-03T03:05:01.671020",
     "exception": false,
     "start_time": "2025-12-03T03:05:01.546519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üéØ B∆Ø·ªöC 6: Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59e30cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T03:05:02.080601Z",
     "iopub.status.busy": "2025-12-03T03:05:02.080206Z",
     "iopub.status.idle": "2025-12-03T03:14:59.962725Z",
     "shell.execute_reply": "2025-12-03T03:14:59.961715Z"
    },
    "papermill": {
     "duration": 598.010603,
     "end_time": "2025-12-03T03:14:59.964183",
     "exception": false,
     "start_time": "2025-12-03T03:05:01.953580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ GENERATING TEST PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "üß¨ Extracting test features (enhanced)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [09:57<00:00, 375.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test features: (224309, 150)\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('üéØ GENERATING TEST PREDICTIONS')\n",
    "print('='*60)\n",
    "\n",
    "# Extract features for test proteins with ENHANCED function\n",
    "print('\\nüß¨ Extracting test features (enhanced)...')\n",
    "\n",
    "test_protein_ids = list(test_sequences.keys())\n",
    "X_test_features = []\n",
    "\n",
    "for pid in tqdm(test_protein_ids, desc='Extracting test features'):\n",
    "    X_test_features.append(extract_features_enhanced(test_sequences[pid]))\n",
    "\n",
    "X_test = np.array(X_test_features)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'‚úÖ Test features: {X_test_scaled.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ec68a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T03:15:00.725587Z",
     "iopub.status.busy": "2025-12-03T03:15:00.725222Z",
     "iopub.status.idle": "2025-12-03T03:15:45.406590Z",
     "shell.execute_reply": "2025-12-03T03:15:45.405360Z"
    },
    "papermill": {
     "duration": 45.063124,
     "end_time": "2025-12-03T03:15:45.408176",
     "exception": false,
     "start_time": "2025-12-03T03:15:00.345052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:43<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions shape: (224309, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions (Batched)\n",
    "print('\\nüöÄ Making predictions...')\n",
    "\n",
    "model.eval()\n",
    "batch_size = 10000\n",
    "all_probs = []\n",
    "\n",
    "for i in tqdm(range(0, len(X_test_scaled), batch_size), desc='Predicting'):\n",
    "    batch_X = X_test_scaled[i:i+batch_size]\n",
    "    X_tensor = torch.FloatTensor(batch_X).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "    all_probs.append(probs)\n",
    "\n",
    "y_test_proba = np.vstack(all_probs)\n",
    "print(f'‚úÖ Predictions shape: {y_test_proba.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213b765",
   "metadata": {
    "papermill": {
     "duration": 0.389886,
     "end_time": "2025-12-03T03:15:46.189607",
     "exception": false,
     "start_time": "2025-12-03T03:15:45.799721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üìù B∆Ø·ªöC 7: Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca8cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T03:15:47.114239Z",
     "iopub.status.busy": "2025-12-03T03:15:47.113958Z",
     "iopub.status.idle": "2025-12-03T03:18:57.992268Z",
     "shell.execute_reply": "2025-12-03T03:18:57.991032Z"
    },
    "papermill": {
     "duration": 191.287079,
     "end_time": "2025-12-03T03:18:57.993850",
     "exception": false,
     "start_time": "2025-12-03T03:15:46.706771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù CREATING SUBMISSION FILE (OPTIMIZED)\n",
      "============================================================\n",
      "\n",
      "üå≥ Applying Smart GO Hierarchy Propagation...\n",
      "   Pre-computing mappings...\n",
      "   Model GO terms: 2,000\n",
      "   Possible output terms: 3,906\n",
      "   Excluded general terms: 11\n",
      "   Base threshold: 0.02\n",
      "   Max per protein: 150\n",
      "   Min per protein: 10\n",
      "   Processing 224,309 proteins...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating submission: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [03:09<00:00, 1180.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Results:\n",
      "   Total predictions: 33,506,076\n",
      "   Unique GO terms: 3,906\n",
      "   Avg per protein: 149.4\n",
      "\n",
      "üîç Top 15 GO terms:\n",
      "    1. GO:0050896: 201,593 (89.9%) - response to stimulus \n",
      "    2. GO:0065007: 190,209 (84.8%) - biological regulation \n",
      "    3. GO:0050789: 182,277 (81.3%) - regulation of biological proce \n",
      "    4. GO:0050794: 173,478 (77.3%) - regulation of cellular process \n",
      "    5. GO:0006950: 170,045 (75.8%) - response to stress \n",
      "    6. GO:0071944: 169,772 (75.7%) - cell periphery \n",
      "    7. GO:0051716: 168,177 (75.0%) - cellular response to stimulus \n",
      "    8. GO:0042802: 154,797 (69.0%) - identical protein binding \n",
      "    9. GO:0032502: 152,239 (67.9%) - developmental process \n",
      "   10. GO:0044238: 149,831 (66.8%) - primary metabolic process \n",
      "   11. GO:0032501: 140,234 (62.5%) - multicellular organismal proce \n",
      "   12. GO:0048856: 140,029 (62.4%) - anatomical structure developme \n",
      "   13. GO:0042221: 138,618 (61.8%) - response to chemical \n",
      "   14. GO:0071840: 135,138 (60.2%) - cellular component organizatio \n",
      "   15. GO:0031974: 128,503 (57.3%) - membrane-enclosed lumen \n",
      "\n",
      "‚úÖ Submission saved: /kaggle/working/submission.tsv\n",
      "   Size: 865.29 MB\n"
     ]
    }
   ],
   "source": [
    "# CREATE SUBMISSION WITH SMARTER GO PROPAGATION\n",
    "print('='*60)\n",
    "print('üìù CREATING SUBMISSION FILE (OPTIMIZED)')\n",
    "print('='*60)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# X√ìA FILE C≈® TR∆Ø·ªöC KHI T·∫†O M·ªöI\n",
    "submission_file = SUBMISSION_PATH / 'submission.tsv'\n",
    "propagated_file = SUBMISSION_PATH / 'submission_propagated.tsv'\n",
    "\n",
    "for f in [submission_file, propagated_file]:\n",
    "    if f.exists():\n",
    "        os.remove(f)\n",
    "        print(f'   üóëÔ∏è ƒê√£ x√≥a file c≈©: {f}')\n",
    "\n",
    "# TERMS TO EXCLUDE OR DOWN-WEIGHT\n",
    "# Root terms -\n",
    "ROOT_TERMS = {\n",
    "    'GO:0008150',  # biological_process\n",
    "    'GO:0003674',  # molecular_function  \n",
    "    'GO:0005575',  # cellular_component\n",
    "}\n",
    "\n",
    "# High-level terms to EXCLUDE\n",
    "EXCLUDE_GENERAL_TERMS = {\n",
    "    'GO:0110165',  # cellular anatomical structure\n",
    "    'GO:0005488',  # binding\n",
    "    'GO:0005623',  # cell\n",
    "    'GO:0044464',  # cell part\n",
    "    'GO:0044424',  # intracellular part\n",
    "    'GO:0005622',  # intracellular\n",
    "    'GO:0009987',  # cellular process\n",
    "    'GO:0008152',  # metabolic process\n",
    "}\n",
    "\n",
    "# Terms to DOWN-WEIGHT significantly\n",
    "DOWNWEIGHT_HEAVY = {\n",
    "    'GO:0005515': 0.3,   # protein binding\n",
    "    'GO:0005737': 0.4,   # cytoplasm\n",
    "    'GO:0005634': 0.5,   # nucleus\n",
    "    'GO:0016020': 0.5,   # membrane\n",
    "    'GO:0005829': 0.4,   # cytosol\n",
    "    'GO:0005886': 0.5,   # plasma membrane\n",
    "    'GO:0043226': 0.5,   # organelle\n",
    "    'GO:0043227': 0.5,   # membrane-bounded organelle\n",
    "    'GO:0043229': 0.5,   # intracellular organelle\n",
    "    'GO:0043231': 0.5,   # intracellular membrane-bounded organelle\n",
    "}\n",
    "\n",
    "# All terms to filter out\n",
    "ALL_EXCLUDE = ROOT_TERMS | EXCLUDE_GENERAL_TERMS\n",
    "\n",
    "#  GO HIERARCHY PROPAGATION - SMARTER VERSION\n",
    "if GO_HIERARCHY_LOADED:\n",
    "    print('\\nüå≥ Applying Smart GO Hierarchy Propagation...')\n",
    "    \n",
    "    # Build IA dictionary\n",
    "    if ia_weights is not None:\n",
    "        ia_dict = dict(zip(ia_weights['go_term'], ia_weights['ia_weight']))\n",
    "        max_ia = max(ia_dict.values()) if ia_dict else 1.0\n",
    "    else:\n",
    "        ia_dict = {}\n",
    "        max_ia = 1.0\n",
    "    \n",
    "    # Build filtered ancestors\n",
    "    print('   Pre-computing mappings...')\n",
    "    term_ancestors_filtered = {}\n",
    "    for term in filtered_go_terms_final:\n",
    "        ancestors = ancestor_cache.get(term, {term})\n",
    "        # Filter out root and general terms\n",
    "        filtered_ancestors = ancestors - ALL_EXCLUDE\n",
    "        term_ancestors_filtered[term] = filtered_ancestors\n",
    "    \n",
    "    # Collect all possible terms\n",
    "    all_terms_in_ancestors = set()\n",
    "    for ancestors in term_ancestors_filtered.values():\n",
    "        all_terms_in_ancestors.update(ancestors)\n",
    "    all_terms_in_ancestors -= ALL_EXCLUDE\n",
    "    \n",
    "    print(f'   Model GO terms: {len(filtered_go_terms_final):,}')\n",
    "    print(f'   Possible output terms: {len(all_terms_in_ancestors):,}')\n",
    "    print(f'   Excluded general terms: {len(ALL_EXCLUDE)}')\n",
    "    \n",
    "    # Precompute term weights (for down-weighting)\n",
    "    term_weights = {}\n",
    "    for t in all_terms_in_ancestors:\n",
    "        if t in DOWNWEIGHT_HEAVY:\n",
    "            term_weights[t] = DOWNWEIGHT_HEAVY[t]\n",
    "        else:\n",
    "            term_weights[t] = 1.0\n",
    "    \n",
    "    # Precompute IA weights\n",
    "    if ia_dict:\n",
    "        term_ia_final = {t: ia_dict.get(t, 1.0) / max_ia for t in all_terms_in_ancestors}\n",
    "    else:\n",
    "        term_ia_final = None\n",
    "    \n",
    "    # DYNAMIC THRESHOLDING\n",
    "    # Use per-protein adaptive thresholding\n",
    "    PROPAGATION_THRESHOLD = 0.02  # Lower base threshold\n",
    "    MAX_PREDS_PROPAGATED = 150    # More predictions\n",
    "    MIN_PREDS_PER_PROTEIN = 10    # Ensure minimum predictions\n",
    "    \n",
    "    print(f'   Base threshold: {PROPAGATION_THRESHOLD}')\n",
    "    print(f'   Max per protein: {MAX_PREDS_PROPAGATED}')\n",
    "    print(f'   Min per protein: {MIN_PREDS_PER_PROTEIN}')\n",
    "    print(f'   Processing {len(test_protein_ids):,} proteins...\\n')\n",
    "    \n",
    "    total_predictions = 0\n",
    "    go_term_counts = {}\n",
    "    \n",
    "    with open(propagated_file, 'w') as f:\n",
    "        for i in tqdm(range(len(test_protein_ids)), desc='Creating submission', mininterval=0.5):\n",
    "            protein_id = test_protein_ids[i]\n",
    "            probs = y_test_proba[i]\n",
    "            \n",
    "            # Get top predictions with LOWER threshold\n",
    "            top_k = 50  # Consider more candidates\n",
    "            top_indices = np.argsort(-probs)[:top_k]\n",
    "            \n",
    "            # Adaptive threshold: use model's top prediction as reference\n",
    "            max_prob = probs[top_indices[0]]\n",
    "            adaptive_threshold = min(PROPAGATION_THRESHOLD, max_prob * 0.3)\n",
    "            \n",
    "            # Filter by threshold\n",
    "            mask = probs[top_indices] >= adaptive_threshold\n",
    "            if mask.sum() >= MIN_PREDS_PER_PROTEIN:\n",
    "                selected_indices = top_indices[mask]\n",
    "            else:\n",
    "                # Keep at least MIN_PREDS_PER_PROTEIN\n",
    "                selected_indices = top_indices[:MIN_PREDS_PER_PROTEIN]\n",
    "            \n",
    "            # Propagate to ancestors with filtering\n",
    "            term_scores = {}\n",
    "            for idx in selected_indices:\n",
    "                term = filtered_go_terms_final[idx]\n",
    "                prob = float(probs[idx])\n",
    "                \n",
    "                # Only propagate if term is in our filtered set\n",
    "                if term in term_ancestors_filtered:\n",
    "                    for ancestor in term_ancestors_filtered[term]:\n",
    "                        if ancestor in ALL_EXCLUDE:\n",
    "                            continue\n",
    "                        \n",
    "                        # Apply down-weight\n",
    "                        weight = term_weights.get(ancestor, 1.0)\n",
    "                        weighted_prob = prob * weight\n",
    "                        \n",
    "                        if ancestor not in term_scores or weighted_prob > term_scores[ancestor]:\n",
    "                            term_scores[ancestor] = weighted_prob\n",
    "            \n",
    "            # Sort and limit predictions\n",
    "            sorted_items = sorted(term_scores.items(), key=lambda x: -x[1])\n",
    "            \n",
    "            # Filter out very low confidence predictions\n",
    "            min_conf = sorted_items[0][1] * 0.1 if sorted_items else 0\n",
    "            sorted_items = [(t, p) for t, p in sorted_items if p >= min_conf]\n",
    "            sorted_items = sorted_items[:MAX_PREDS_PROPAGATED]\n",
    "            \n",
    "            # Write predictions\n",
    "            for term, prob in sorted_items:\n",
    "                # Apply IA weight adjustment\n",
    "                if term_ia_final and term in term_ia_final:\n",
    "                    ia_w = term_ia_final[term]\n",
    "                    # Higher IA = more informative = boost slightly\n",
    "                    final_prob = prob * (0.8 + 0.4 * ia_w)\n",
    "                else:\n",
    "                    final_prob = prob\n",
    "                \n",
    "                # Clamp probability\n",
    "                final_prob = min(1.0, max(0.001, final_prob))\n",
    "                \n",
    "                f.write(f'{protein_id}\\t{term}\\t{final_prob:.6f}\\n')\n",
    "                total_predictions += 1\n",
    "                go_term_counts[term] = go_term_counts.get(term, 0) + 1\n",
    "    \n",
    "    print(f'\\nüìä Results:')\n",
    "    print(f'   Total predictions: {total_predictions:,}')\n",
    "    print(f'   Unique GO terms: {len(go_term_counts):,}')\n",
    "    print(f'   Avg per protein: {total_predictions / len(test_protein_ids):.1f}')\n",
    "    \n",
    "    # Show distribution of top terms\n",
    "    print(f'\\nüîç Top 15 GO terms:')\n",
    "    sorted_terms = sorted(go_term_counts.items(), key=lambda x: -x[1])[:15]\n",
    "    for i, (term, count) in enumerate(sorted_terms, 1):\n",
    "        name = go_names.get(term, '')[:30]\n",
    "        pct = 100 * count / len(test_protein_ids)\n",
    "        # Check if down-weighted\n",
    "        weight = term_weights.get(term, 1.0)\n",
    "        weight_str = f'(w={weight})' if weight < 1.0 else ''\n",
    "        print(f'   {i:2d}. {term}: {count:,} ({pct:.1f}%) - {name} {weight_str}')\n",
    "    \n",
    "    # Check if too many general terms\n",
    "    general_in_top = sum(1 for t, _ in sorted_terms[:10] if t in DOWNWEIGHT_HEAVY)\n",
    "    if general_in_top > 5:\n",
    "        print(f'\\n   ‚ö†Ô∏è Warning: {general_in_top}/10 top terms are general (down-weighted)')\n",
    "    \n",
    "    # Copy to final submission\n",
    "    shutil.copy(propagated_file, submission_file)\n",
    "    \n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è GO hierarchy not loaded - creating simple submission...')\n",
    "    \n",
    "    SIMPLE_THRESHOLD = 0.02\n",
    "    MAX_PREDS = 150\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with open(submission_file, 'w') as f:\n",
    "        for i in tqdm(range(len(test_protein_ids)), desc='Creating submission'):\n",
    "            protein_id = test_protein_ids[i]\n",
    "            probs = y_test_proba[i]\n",
    "            \n",
    "            top_indices = np.argsort(-probs)[:MAX_PREDS]\n",
    "            mask = probs[top_indices] >= SIMPLE_THRESHOLD\n",
    "            if mask.sum() >= 10:\n",
    "                top_indices = top_indices[mask]\n",
    "            else:\n",
    "                top_indices = top_indices[:10]\n",
    "            \n",
    "            for idx in top_indices:\n",
    "                term = filtered_go_terms_final[idx]\n",
    "                prob = float(probs[idx])\n",
    "                f.write(f'{protein_id}\\t{term}\\t{prob:.6f}\\n')\n",
    "                total_predictions += 1\n",
    "    \n",
    "    print(f'   Total predictions: {total_predictions:,}')\n",
    "\n",
    "# Verify file\n",
    "file_size = os.path.getsize(submission_file) / (1024 * 1024)\n",
    "print(f'\\n‚úÖ Submission saved: {submission_file}')\n",
    "print(f'   Size: {file_size:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aef1f844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T03:18:58.791358Z",
     "iopub.status.busy": "2025-12-03T03:18:58.791074Z",
     "iopub.status.idle": "2025-12-03T03:19:10.950928Z",
     "shell.execute_reply": "2025-12-03T03:19:10.949739Z"
    },
    "papermill": {
     "duration": 12.558898,
     "end_time": "2025-12-03T03:19:10.952493",
     "exception": false,
     "start_time": "2025-12-03T03:18:58.393595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Submission Preview:\n",
      "protein_id    go_term  confidence\n",
      "A0A0C5B5G6 GO:0003677    0.482135\n",
      "A0A0C5B5G6 GO:0003676    0.510117\n",
      "A0A0C5B5G6 GO:0065007    0.473632\n",
      "A0A0C5B5G6 GO:0080090    0.462964\n",
      "A0A0C5B5G6 GO:0050789    0.458164\n",
      "A0A0C5B5G6 GO:0048522    0.458286\n",
      "A0A0C5B5G6 GO:0051252    0.459022\n",
      "A0A0C5B5G6 GO:0006357    0.468526\n",
      "A0A0C5B5G6 GO:1902680    0.457236\n",
      "A0A0C5B5G6 GO:0019222    0.471166\n",
      "\n",
      "üèÜ Top GO Terms in submission:\n",
      "   1. GO:0050896: 201,593 (response to stimulus...)\n",
      "   2. GO:0065007: 190,209 (biological regulation...)\n",
      "   3. GO:0050789: 182,277 (regulation of biological process...)\n",
      "   4. GO:0050794: 173,478 (regulation of cellular process...)\n",
      "   5. GO:0006950: 170,045 (response to stress...)\n",
      "   6. GO:0071944: 169,772 (cell periphery...)\n",
      "   7. GO:0051716: 168,177 (cellular response to stimulus...)\n",
      "   8. GO:0042802: 154,797 (identical protein binding...)\n",
      "   9. GO:0032502: 152,239 (developmental process...)\n",
      "   10. GO:0044238: 149,831 (primary metabolic process...)\n"
     ]
    }
   ],
   "source": [
    "# Preview submission\n",
    "print('\\nüìÑ Submission Preview:')\n",
    "preview_df = pd.read_csv(submission_file, sep='\\t', header=None, names=['protein_id', 'go_term', 'confidence'], nrows=10)\n",
    "print(preview_df.to_string(index=False))\n",
    "\n",
    "print('\\nüèÜ Top GO Terms in submission:')\n",
    "# Read just go_term column for stats\n",
    "go_counts = pd.read_csv(submission_file, sep='\\t', header=None, usecols=[1], names=['go_term'])['go_term'].value_counts().head(10)\n",
    "for i, (term, count) in enumerate(go_counts.items(), 1):\n",
    "    name = go_names.get(term, 'Unknown') if GO_HIERARCHY_LOADED else ''\n",
    "    print(f'   {i}. {term}: {count:,} ({name[:40]}...)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1217b15",
   "metadata": {
    "papermill": {
     "duration": 0.510583,
     "end_time": "2025-12-03T03:19:11.860330",
     "exception": false,
     "start_time": "2025-12-03T03:19:11.349747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ‚úÖ B∆Ø·ªöC 8: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26777678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T03:19:12.646796Z",
     "iopub.status.busy": "2025-12-03T03:19:12.646010Z",
     "iopub.status.idle": "2025-12-03T03:19:12.653192Z",
     "shell.execute_reply": "2025-12-03T03:19:12.651793Z"
    },
    "papermill": {
     "duration": 0.40117,
     "end_time": "2025-12-03T03:19:12.655053",
     "exception": false,
     "start_time": "2025-12-03T03:19:12.253883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ PIPELINE COMPLETE!\n",
      "============================================================\n",
      "\n",
      "üìä SUMMARY:\n",
      "==================================================\n",
      "\n",
      "üìÅ Data:\n",
      "   ‚Ä¢ Training proteins: 82,404\n",
      "   ‚Ä¢ Test proteins: 224,309\n",
      "   ‚Ä¢ GO terms trained: 2,000\n",
      "\n",
      "üß¨ Features: 150 dimensions\n",
      "   ‚Ä¢ AAC, Physicochemical, Dipeptides, PseAAC\n",
      "\n",
      "ü§ñ Model: Neural Network\n",
      "   ‚Ä¢ Hidden: 768, ResBlocks: 4\n",
      "   ‚Ä¢ Focal Loss + IA Weights\n",
      "   ‚Ä¢ Training time: 60.1 min\n",
      "\n",
      "üå≥ GO Propagation: ‚úÖ ENABLED\n",
      "\n",
      "üìù Submission:\n",
      "   ‚Ä¢ File: /kaggle/working/submission.tsv\n",
      "   ‚Ä¢ Size: 865.3 MB\n",
      "\n",
      "==================================================\n",
      "‚úÖ Ready for Kaggle submission!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('üéâ PIPELINE COMPLETE!')\n",
    "print('='*60)\n",
    "\n",
    "# Get stats from file\n",
    "import os\n",
    "file_size = os.path.getsize(submission_file) / (1024 * 1024)  # MB\n",
    "\n",
    "print(f'''\n",
    "üìä SUMMARY:\n",
    "{'='*50}\n",
    "\n",
    "üìÅ Data:\n",
    "   ‚Ä¢ Training proteins: {len(valid_proteins):,}\n",
    "   ‚Ä¢ Test proteins: {len(test_protein_ids):,}\n",
    "   ‚Ä¢ GO terms trained: {len(filtered_go_terms_final):,}\n",
    "\n",
    "üß¨ Features: {X_scaled.shape[1]} dimensions\n",
    "   ‚Ä¢ AAC, Physicochemical, Dipeptides, PseAAC\n",
    "\n",
    "ü§ñ Model: Neural Network\n",
    "   ‚Ä¢ Hidden: {HIDDEN_DIM}, ResBlocks: 4\n",
    "   ‚Ä¢ Focal Loss + IA Weights\n",
    "   ‚Ä¢ Training time: {training_time/60:.1f} min\n",
    "\n",
    "üå≥ GO Propagation: {\"‚úÖ ENABLED\" if GO_HIERARCHY_LOADED else \"‚ùå\"}\n",
    "\n",
    "üìù Submission:\n",
    "   ‚Ä¢ File: {submission_file}\n",
    "   ‚Ä¢ Size: {file_size:.1f} MB\n",
    "\n",
    "{'='*50}\n",
    "‚úÖ Ready for Kaggle submission!\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8794947,
     "sourceId": 13812159,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4933.142054,
   "end_time": "2025-12-03T03:19:16.762779",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-03T01:57:03.620725",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
